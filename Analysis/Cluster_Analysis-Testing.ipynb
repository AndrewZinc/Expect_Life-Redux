{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da1f77a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "  var py_version = '3.3.4'.replace('rc', '-rc.').replace('.dev', '-dev.');\n",
       "  var reloading = false;\n",
       "  var Bokeh = root.Bokeh;\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks;\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "    if (js_modules == null) js_modules = [];\n",
       "    if (js_exports == null) js_exports = {};\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    if (!reloading) {\n",
       "      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    }\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "    window._bokeh_on_load = on_load\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n",
       "      require([\"jspanel\"], function(jsPanel) {\n",
       "\twindow.jsPanel = jsPanel\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-modal\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-tooltip\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-hint\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-layout\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-contextmenu\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"jspanel-dock\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"gridstack\"], function(GridStack) {\n",
       "\twindow.GridStack = GridStack\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"notyf\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      root._bokeh_is_loading = css_urls.length + 9;\n",
       "    } else {\n",
       "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n",
       "    }\n",
       "\n",
       "    var existing_stylesheets = []\n",
       "    var links = document.getElementsByTagName('link')\n",
       "    for (var i = 0; i < links.length; i++) {\n",
       "      var link = links[i]\n",
       "      if (link.href != null) {\n",
       "\texisting_stylesheets.push(link.href)\n",
       "      }\n",
       "    }\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      if (existing_stylesheets.indexOf(url) !== -1) {\n",
       "\ton_load()\n",
       "\tcontinue;\n",
       "      }\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/1.3.8/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.holoviz.org/panel/1.3.8/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    var existing_scripts = []\n",
       "    var scripts = document.getElementsByTagName('script')\n",
       "    for (var i = 0; i < scripts.length; i++) {\n",
       "      var script = scripts[i]\n",
       "      if (script.src != null) {\n",
       "\texisting_scripts.push(script.src)\n",
       "      }\n",
       "    }\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (var i = 0; i < js_modules.length; i++) {\n",
       "      var url = js_modules[i];\n",
       "      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (const name in js_exports) {\n",
       "      var url = js_exports[name];\n",
       "      if (skip.indexOf(url) >= 0 || root[name] != null) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      element.textContent = `\n",
       "      import ${name} from \"${url}\"\n",
       "      window.${name} = ${name}\n",
       "      window._bokeh_on_load()\n",
       "      `\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    if (!js_urls.length && !js_modules.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.4.min.js\", \"https://cdn.holoviz.org/panel/1.3.8/dist/panel.min.js\"];\n",
       "  var js_modules = [];\n",
       "  var js_exports = {};\n",
       "  var css_urls = [];\n",
       "  var inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "\ttry {\n",
       "          inline_js[i].call(root, root.Bokeh);\n",
       "\t} catch(e) {\n",
       "\t  if (!reloading) {\n",
       "\t    throw e;\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "      // Cache old bokeh versions\n",
       "      if (Bokeh != undefined && !reloading) {\n",
       "\tvar NewBokeh = root.Bokeh;\n",
       "\tif (Bokeh.versions === undefined) {\n",
       "\t  Bokeh.versions = new Map();\n",
       "\t}\n",
       "\tif (NewBokeh.version !== Bokeh.version) {\n",
       "\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n",
       "\t}\n",
       "\troot.Bokeh = Bokeh;\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "    root._bokeh_is_initializing = false\n",
       "  }\n",
       "\n",
       "  function load_or_wait() {\n",
       "    // Implement a backoff loop that tries to ensure we do not load multiple\n",
       "    // versions of Bokeh and its dependencies at the same time.\n",
       "    // In recent versions we use the root._bokeh_is_initializing flag\n",
       "    // to determine whether there is an ongoing attempt to initialize\n",
       "    // bokeh, however for backward compatibility we also try to ensure\n",
       "    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n",
       "    // before older versions are fully initialized.\n",
       "    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n",
       "      root._bokeh_is_initializing = false;\n",
       "      root._bokeh_onload_callbacks = undefined;\n",
       "      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n",
       "      load_or_wait();\n",
       "    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n",
       "      setTimeout(load_or_wait, 100);\n",
       "    } else {\n",
       "      root._bokeh_is_initializing = true\n",
       "      root._bokeh_onload_callbacks = []\n",
       "      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n",
       "      if (!reloading && !bokeh_loaded) {\n",
       "\troot.Bokeh = undefined;\n",
       "      }\n",
       "      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n",
       "\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "\trun_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  // Give older versions of the autoload script a head-start to ensure\n",
       "  // they initialize before we start loading newer version.\n",
       "  setTimeout(load_or_wait, 100)\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.3.4'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.3.8/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.8/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.8/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.4.min.js\", \"https://cdn.holoviz.org/panel/1.3.8/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='p1002'>\n",
       "  <div id=\"b421bb06-69ca-4eab-a46b-a6af876f02b6\" data-root-id=\"p1002\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"3c8737a7-2d05-4cad-9671-a4ac7a5939d7\":{\"version\":\"3.3.4\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"p1002\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"p1003\",\"attributes\":{\"plot_id\":\"p1002\",\"comm_id\":\"a411bbcd5a0b498b9135a35f9b685392\",\"client_comm_id\":\"1531eccaf0804c09bb1fc3484c042561\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"3c8737a7-2d05-4cad-9671-a4ac7a5939d7\",\"roots\":{\"p1002\":\"b421bb06-69ca-4eab-a46b-a6af876f02b6\"},\"root_ids\":[\"p1002\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "p1002"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the dependencies\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "from functools import reduce\n",
    "from itertools import combinations\n",
    "from joblib import Parallel, delayed\n",
    "from pymongo import MongoClient\n",
    "import pymongoarrow as pma\n",
    "from pymongoarrow.api import write\n",
    "import numba\n",
    "from numba.typed import List\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import hvplot.pandas\n",
    "from matplotlib import pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Suppress YData profile report generation warnings - no actual problems to resolve.\n",
    "from warnings import simplefilter \n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14229f21-8a36-4a95-be30-f37f45d8a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply latest settings for Pandas\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018184e0-2afd-4a76-8fae-42b95b75a610",
   "metadata": {},
   "source": [
    "## Create a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc68340-18a0-4b52-a32d-f79c23c9239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Create a test dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=50, n_informative=5, n_redundant=45, n_repeated=0, n_classes=5, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b3cebc5-516b-435a-9255-34ab646d6187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.079844</td>\n",
       "      <td>4.082231</td>\n",
       "      <td>-0.928285</td>\n",
       "      <td>-1.406834</td>\n",
       "      <td>3.821611</td>\n",
       "      <td>-1.454153</td>\n",
       "      <td>0.558609</td>\n",
       "      <td>-2.833646</td>\n",
       "      <td>1.207253</td>\n",
       "      <td>-2.156476</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.225367</td>\n",
       "      <td>-1.002860</td>\n",
       "      <td>-3.506234</td>\n",
       "      <td>2.659189</td>\n",
       "      <td>1.335290</td>\n",
       "      <td>-0.193650</td>\n",
       "      <td>-2.842733</td>\n",
       "      <td>0.281155</td>\n",
       "      <td>-0.036362</td>\n",
       "      <td>-0.106037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.168850</td>\n",
       "      <td>-2.940722</td>\n",
       "      <td>-0.120378</td>\n",
       "      <td>1.911458</td>\n",
       "      <td>-4.086751</td>\n",
       "      <td>3.005609</td>\n",
       "      <td>1.061180</td>\n",
       "      <td>-0.546356</td>\n",
       "      <td>-3.393992</td>\n",
       "      <td>1.606306</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.673654</td>\n",
       "      <td>1.082064</td>\n",
       "      <td>2.408369</td>\n",
       "      <td>-0.871287</td>\n",
       "      <td>-1.531216</td>\n",
       "      <td>-1.078653</td>\n",
       "      <td>0.172315</td>\n",
       "      <td>0.802343</td>\n",
       "      <td>3.421586</td>\n",
       "      <td>2.074265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.118903</td>\n",
       "      <td>-2.292353</td>\n",
       "      <td>-1.772501</td>\n",
       "      <td>2.180867</td>\n",
       "      <td>0.157815</td>\n",
       "      <td>-1.789476</td>\n",
       "      <td>0.766534</td>\n",
       "      <td>0.560211</td>\n",
       "      <td>1.261955</td>\n",
       "      <td>0.183973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142958</td>\n",
       "      <td>-0.866399</td>\n",
       "      <td>-0.362532</td>\n",
       "      <td>-0.202082</td>\n",
       "      <td>0.837790</td>\n",
       "      <td>-1.247566</td>\n",
       "      <td>-1.536458</td>\n",
       "      <td>-0.201039</td>\n",
       "      <td>-1.277475</td>\n",
       "      <td>1.892091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.059561</td>\n",
       "      <td>-3.351614</td>\n",
       "      <td>-0.785609</td>\n",
       "      <td>1.608111</td>\n",
       "      <td>-2.891339</td>\n",
       "      <td>-1.140441</td>\n",
       "      <td>1.951887</td>\n",
       "      <td>-0.236173</td>\n",
       "      <td>-0.437770</td>\n",
       "      <td>2.257933</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.997639</td>\n",
       "      <td>0.278740</td>\n",
       "      <td>0.688077</td>\n",
       "      <td>0.335144</td>\n",
       "      <td>-0.791574</td>\n",
       "      <td>-2.209142</td>\n",
       "      <td>-1.045707</td>\n",
       "      <td>-1.063227</td>\n",
       "      <td>0.425318</td>\n",
       "      <td>2.813590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.027416</td>\n",
       "      <td>1.913728</td>\n",
       "      <td>0.206195</td>\n",
       "      <td>-2.121960</td>\n",
       "      <td>1.021316</td>\n",
       "      <td>-3.534001</td>\n",
       "      <td>1.676292</td>\n",
       "      <td>-1.997667</td>\n",
       "      <td>1.832500</td>\n",
       "      <td>0.889809</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.110175</td>\n",
       "      <td>-0.141692</td>\n",
       "      <td>-2.646638</td>\n",
       "      <td>2.944468</td>\n",
       "      <td>-0.104274</td>\n",
       "      <td>-1.446555</td>\n",
       "      <td>-1.741149</td>\n",
       "      <td>-2.128734</td>\n",
       "      <td>-1.182597</td>\n",
       "      <td>0.541159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2.468921</td>\n",
       "      <td>6.487245</td>\n",
       "      <td>-0.783022</td>\n",
       "      <td>0.085301</td>\n",
       "      <td>5.309236</td>\n",
       "      <td>2.501753</td>\n",
       "      <td>-0.682770</td>\n",
       "      <td>-3.919702</td>\n",
       "      <td>0.031491</td>\n",
       "      <td>-3.261642</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.955460</td>\n",
       "      <td>-0.274155</td>\n",
       "      <td>-3.475691</td>\n",
       "      <td>1.220751</td>\n",
       "      <td>0.361299</td>\n",
       "      <td>0.269580</td>\n",
       "      <td>-2.155254</td>\n",
       "      <td>2.413773</td>\n",
       "      <td>2.309532</td>\n",
       "      <td>-2.087254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.568388</td>\n",
       "      <td>0.673732</td>\n",
       "      <td>-0.928115</td>\n",
       "      <td>1.489449</td>\n",
       "      <td>0.752600</td>\n",
       "      <td>-2.087146</td>\n",
       "      <td>2.086560</td>\n",
       "      <td>-2.462543</td>\n",
       "      <td>1.801995</td>\n",
       "      <td>1.800496</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.962576</td>\n",
       "      <td>0.328587</td>\n",
       "      <td>-2.421311</td>\n",
       "      <td>1.516172</td>\n",
       "      <td>-1.255831</td>\n",
       "      <td>-3.225189</td>\n",
       "      <td>-2.230410</td>\n",
       "      <td>-1.359061</td>\n",
       "      <td>-0.114012</td>\n",
       "      <td>1.170844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.452091</td>\n",
       "      <td>-1.445143</td>\n",
       "      <td>0.271223</td>\n",
       "      <td>-1.897568</td>\n",
       "      <td>-1.823580</td>\n",
       "      <td>-1.299518</td>\n",
       "      <td>0.661519</td>\n",
       "      <td>0.482176</td>\n",
       "      <td>-0.896208</td>\n",
       "      <td>0.141058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.875607</td>\n",
       "      <td>-0.393528</td>\n",
       "      <td>0.711648</td>\n",
       "      <td>1.156760</td>\n",
       "      <td>0.846857</td>\n",
       "      <td>0.561399</td>\n",
       "      <td>-0.248216</td>\n",
       "      <td>-0.770103</td>\n",
       "      <td>-0.226960</td>\n",
       "      <td>1.482281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>4.512252</td>\n",
       "      <td>-0.314382</td>\n",
       "      <td>-4.586837</td>\n",
       "      <td>-0.767961</td>\n",
       "      <td>4.679979</td>\n",
       "      <td>-2.145382</td>\n",
       "      <td>-0.849245</td>\n",
       "      <td>0.728320</td>\n",
       "      <td>-0.187187</td>\n",
       "      <td>-7.886254</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.245725</td>\n",
       "      <td>-4.996025</td>\n",
       "      <td>-1.804474</td>\n",
       "      <td>1.994489</td>\n",
       "      <td>7.835717</td>\n",
       "      <td>3.577882</td>\n",
       "      <td>-5.327821</td>\n",
       "      <td>3.437536</td>\n",
       "      <td>-1.587368</td>\n",
       "      <td>3.600663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1.907637</td>\n",
       "      <td>0.771050</td>\n",
       "      <td>-0.831644</td>\n",
       "      <td>-1.287527</td>\n",
       "      <td>1.754592</td>\n",
       "      <td>-2.170435</td>\n",
       "      <td>0.175911</td>\n",
       "      <td>-0.197949</td>\n",
       "      <td>1.030527</td>\n",
       "      <td>-1.605901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122734</td>\n",
       "      <td>-1.307149</td>\n",
       "      <td>-1.443372</td>\n",
       "      <td>1.492301</td>\n",
       "      <td>1.948312</td>\n",
       "      <td>0.575648</td>\n",
       "      <td>-1.613453</td>\n",
       "      <td>-0.155471</td>\n",
       "      <td>-1.363791</td>\n",
       "      <td>0.688680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0     3.079844   4.082231  -0.928285  -1.406834   3.821611  -1.454153   \n",
       "1    -2.168850  -2.940722  -0.120378   1.911458  -4.086751   3.005609   \n",
       "2    -0.118903  -2.292353  -1.772501   2.180867   0.157815  -1.789476   \n",
       "3    -1.059561  -3.351614  -0.785609   1.608111  -2.891339  -1.140441   \n",
       "4     2.027416   1.913728   0.206195  -2.121960   1.021316  -3.534001   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "995   2.468921   6.487245  -0.783022   0.085301   5.309236   2.501753   \n",
       "996   0.568388   0.673732  -0.928115   1.489449   0.752600  -2.087146   \n",
       "997   0.452091  -1.445143   0.271223  -1.897568  -1.823580  -1.299518   \n",
       "998   4.512252  -0.314382  -4.586837  -0.767961   4.679979  -2.145382   \n",
       "999   1.907637   0.771050  -0.831644  -1.287527   1.754592  -2.170435   \n",
       "\n",
       "     feature_6  feature_7  feature_8  feature_9  ...  feature_40  feature_41  \\\n",
       "0     0.558609  -2.833646   1.207253  -2.156476  ...   -2.225367   -1.002860   \n",
       "1     1.061180  -0.546356  -3.393992   1.606306  ...   -3.673654    1.082064   \n",
       "2     0.766534   0.560211   1.261955   0.183973  ...    0.142958   -0.866399   \n",
       "3     1.951887  -0.236173  -0.437770   2.257933  ...   -1.997639    0.278740   \n",
       "4     1.676292  -1.997667   1.832500   0.889809  ...   -1.110175   -0.141692   \n",
       "..         ...        ...        ...        ...  ...         ...         ...   \n",
       "995  -0.682770  -3.919702   0.031491  -3.261642  ...   -2.955460   -0.274155   \n",
       "996   2.086560  -2.462543   1.801995   1.800496  ...   -1.962576    0.328587   \n",
       "997   0.661519   0.482176  -0.896208   0.141058  ...   -0.875607   -0.393528   \n",
       "998  -0.849245   0.728320  -0.187187  -7.886254  ...   -2.245725   -4.996025   \n",
       "999   0.175911  -0.197949   1.030527  -1.605901  ...   -0.122734   -1.307149   \n",
       "\n",
       "     feature_42  feature_43  feature_44  feature_45  feature_46  feature_47  \\\n",
       "0     -3.506234    2.659189    1.335290   -0.193650   -2.842733    0.281155   \n",
       "1      2.408369   -0.871287   -1.531216   -1.078653    0.172315    0.802343   \n",
       "2     -0.362532   -0.202082    0.837790   -1.247566   -1.536458   -0.201039   \n",
       "3      0.688077    0.335144   -0.791574   -2.209142   -1.045707   -1.063227   \n",
       "4     -2.646638    2.944468   -0.104274   -1.446555   -1.741149   -2.128734   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "995   -3.475691    1.220751    0.361299    0.269580   -2.155254    2.413773   \n",
       "996   -2.421311    1.516172   -1.255831   -3.225189   -2.230410   -1.359061   \n",
       "997    0.711648    1.156760    0.846857    0.561399   -0.248216   -0.770103   \n",
       "998   -1.804474    1.994489    7.835717    3.577882   -5.327821    3.437536   \n",
       "999   -1.443372    1.492301    1.948312    0.575648   -1.613453   -0.155471   \n",
       "\n",
       "     feature_48  feature_49  \n",
       "0     -0.036362   -0.106037  \n",
       "1      3.421586    2.074265  \n",
       "2     -1.277475    1.892091  \n",
       "3      0.425318    2.813590  \n",
       "4     -1.182597    0.541159  \n",
       "..          ...         ...  \n",
       "995    2.309532   -2.087254  \n",
       "996   -0.114012    1.170844  \n",
       "997   -0.226960    1.482281  \n",
       "998   -1.587368    3.600663  \n",
       "999   -1.363791    0.688680  \n",
       "\n",
       "[1000 rows x 50 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(X)\n",
    "test_df = test_df.add_prefix('feature_')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7426e97-0ed0-4c26-a5b1-46ca3a417a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fb4a972-e7fe-4ca3-b1d3-d866f4715594",
   "metadata": {},
   "source": [
    "## Principal Feature Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2bf07-10e8-4c74-a7db-98a407b85926",
   "metadata": {},
   "source": [
    "#### Define functions to select dataset features that provide relevant information for clustering. \n",
    "##### Only important features are used to compute clusters from the complete (non-pca) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b942d4e6-7ad2-4b99-ac0f-8b8fca9d1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper function - Custom Mean processing to override limitations of Numba compatibility with Numpy features\n",
    "@numba.jit(nopython=True)\n",
    "def custom_mean(arr):\n",
    "    if arr.ndim == 1:\n",
    "        return arr.mean()\n",
    "    elif arr.ndim == 2:\n",
    "        # For 2D arrays, manually compute the mean of each column.\n",
    "        means = np.zeros(arr.shape[1])\n",
    "        for i in range(arr.shape[1]):\n",
    "            means[i] = arr[:, i].mean()\n",
    "        return means\n",
    "    else:\n",
    "        raise ValueError(\"Invalid array dimensions for custom_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba54052f-8069-4a6f-9ba9-b32b80222f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper function - Calinski Harbasz Score Calculation\n",
    "def calculate_calinski_harbasz(np_array, labels):\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        c_h = calinski_harabasz_score(np_array, labels)\n",
    "        calinski_harbasz = log_scale_value(c_h)\n",
    "        #print(f'    calinski_harbasz index = {calinski_harbasz}')\n",
    "        return calinski_harbasz\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34286573-3ba9-42cc-86d0-9e23e73f708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper function - Davies-Bouldin Score Calculation\n",
    "def calculate_davies_bouldin(np_array, labels):\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        davies_bouldin = davies_bouldin_score(np_array, labels)\n",
    "        #print(f'   davies_bouldin score = {davies_bouldin}')\n",
    "        return davies_bouldin\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7131fdb1-e0c3-4534-abd8-641895262dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper function - Silhouette Coefficient Calculation\n",
    "def calculate_silhouette(np_array, labels):\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        silhouette_val = silhouette_score(np_array, labels)\n",
    "        #print(f' Silhouette Score = {silhouette_val}')\n",
    "        return silhouette_val\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6fad2f8-0256-472a-85e1-5ae23b2a65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper function - Scatter Separability Calculation\n",
    "def calculate_scatter_separability(np_array, labels):\n",
    "    \"\"\"\n",
    "    Calculates the Scatter Separability (SSC) between clusters in a NumPy array.\n",
    "\n",
    "    Args:\n",
    "        np_array: The NumPy array containing the data points.\n",
    "        labels: The cluster labels for each data point.\n",
    "\n",
    "    Returns:\n",
    "        The Scatter Separability score (SSC).\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_features = np_array.shape[1]\n",
    "    overall_mean = custom_mean(np_array)\n",
    "\n",
    "    S_w = np.zeros((n_features, n_features))\n",
    "    S_b = np.zeros((n_features, n_features))\n",
    "\n",
    "    for label in unique_labels:\n",
    "        X_k = np_array[labels == label]\n",
    "        mean_k = custom_mean(X_k)\n",
    "        if mean_k.ndim == 1:\n",
    "            mean_k = mean_k[:, None]  # Ensure mean_k is a column vector\n",
    "        n_k = X_k.shape[0]  # Number of samples in current class\n",
    "        mean_diff = mean_k - overall_mean[:, None]  # Ensure mean_diff is correctly shaped\n",
    "    \n",
    "        S_w_k = np.cov(X_k, rowvar=False, bias=True) * (n_k - 1)  # Compute within-class scatter for class k\n",
    "        S_w += S_w_k\n",
    "    \n",
    "        mean_diff = mean_diff.reshape(-1, 1)  # Reshape mean_diff as column vector if not already\n",
    "        S_b += n_k * np.dot(mean_diff, mean_diff.T)  # Correct outer product computation\n",
    "\n",
    "    # Handle singular S_w by adding a small identity matrix to ensure invertibility\n",
    "    if np.linalg.cond(S_w) > 1e10:\n",
    "        S_w += np.eye(S_w.shape[0]) * 1e-4\n",
    "\n",
    "    ssc = np.trace(np.linalg.inv(S_w).dot(S_b))\n",
    "    final_ssc = log_scale_value(ssc)\n",
    "    #print(f'+ computed SSC - value = {final_ssc} ')\n",
    "    return final_ssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d6d94d-d011-48b5-87f8-f3e7a2ec5b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper function - Performs Logarithmic scaling of cluster quality score metrics that have unbounded positive ranges - Numba acceleration\n",
    "@numba.jit(nopython=True)\n",
    "def log_scale_value(value, offset=1):\n",
    "    \"\"\"\n",
    "    Applies a logarithmic scaling to a value.\n",
    "    \n",
    "    Parameters:\n",
    "    - value: The positive metric value to be scaled.\n",
    "    - offset: A small positive value to avoid log(0) when the metric is zero.\n",
    "    - scale_max: An upper limit to scale the logarithmic value to, for normalization.\n",
    "    \n",
    "    Returns:\n",
    "    - scaled_value: The logarithmically scaled value, normalized to the range [0, scale_max].\n",
    "    \"\"\"\n",
    "    # Apply logarithmic scaling\n",
    "    log_scaled_value = np.log(value + offset)\n",
    "    \n",
    "    # TBD, normalize the log-scaled value to a specific range, e.g., [0, scale_max]\n",
    "    # if/when implemented, add \", scale_max=1000\" as a function parameter\n",
    "    \n",
    "    return log_scaled_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a52cc396-7659-4138-a334-246467295a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper function - Normalization of criterion values to remove bias due to number of clusters - Numba acceleration\n",
    "@numba.jit(nopython=True)\n",
    "def cross_projection_normalization(clustering_medoids, scatter_criteria_score, silhouette_criteria_score, davies_bouldin_score, calinski_harbasz_index):\n",
    "    n_clusters = len(clustering_medoids)\n",
    "    projections = np.zeros((n_clusters, n_clusters))\n",
    "\n",
    "    for j in range(n_clusters):\n",
    "        for k in range(j + 1, n_clusters):\n",
    "            medoid_j = clustering_medoids[j]\n",
    "            medoid_k = clustering_medoids[k]\n",
    "            distance = np.linalg.norm(medoid_j - medoid_k)\n",
    "            projections[j][k] = distance\n",
    "            projections[k][j] = distance\n",
    "\n",
    "   # Check if all distances are zero\n",
    "    if np.all(projections == 0):\n",
    "        #print(\"CNP - All pairwise distances are zero! Cannot compute Normalized Score.\")\n",
    "        return 0 \n",
    "    \n",
    "    # Flatten the array and filter non-zero distances\n",
    "    flat_projections = projections.ravel()\n",
    "    non_zero_projections = flat_projections[flat_projections > 0]\n",
    "    \n",
    "    if non_zero_projections.size == 0:\n",
    "        #print(\"CNP - non_zero_projections is empty. Cannot compute Normalized Score.\")\n",
    "        return 0\n",
    "\n",
    "    # Calculate mean of non-zero distances\n",
    "    mean_projection = np.mean(non_zero_projections)\n",
    "\n",
    "    # Normalizing the criteria scores with the mean of projections\n",
    "    # Adjusting the formula to consider Davies-Bouldin Score. Recall: For Davies-Bouldin, lower is better.\n",
    "    # We add 1 to both the numerator and denominator to ensure it doesn't lead to division by zero or negative values.\n",
    "\n",
    "    # Combined normalization factor incorporates all metrics.\n",
    "    normalization_denominator = (1 + mean_projection + davies_bouldin_score) \n",
    "    normalization_numerator = (1 + scatter_criteria_score + silhouette_criteria_score + calinski_harbasz_index)\n",
    "    normalized_score = normalization_numerator / normalization_denominator\n",
    "\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dd5e799-345e-477d-a30a-d6b7c4a936b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Primary function - calls functions to generate cluster metrics in parallel\n",
    "def score_subset_clusters(subset_array, np_array, cluster_labels, clustering_medoids):\n",
    "    # Define tasks to be executed in parallel\n",
    "    tasks = [delayed(calculate_scatter_separability)(subset_array, cluster_labels),\n",
    "             delayed(calculate_silhouette)(subset_array, cluster_labels),\n",
    "             delayed(calculate_davies_bouldin)(subset_array, cluster_labels),\n",
    "             delayed(calculate_calinski_harbasz)(subset_array, cluster_labels)]\n",
    "    \n",
    "    # Execute tasks in parallel and unpack results\n",
    "    scatter_separability, silhouette_score, davies_bouldin_score, calinski_harbasz_index = Parallel(n_jobs=4)(tasks)\n",
    "    \n",
    "    # Pass the results to the normalization function\n",
    "    normalized_score = cross_projection_normalization(clustering_medoids, scatter_separability, silhouette_score, davies_bouldin_score, calinski_harbasz_index)\n",
    "\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcd7d156-152b-4a2d-a09c-3d75a0c9002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Primary function - Initializes the clustering algorithm\n",
    "def initialize_clustering_instance(clustering_algorithm, rng, flag, common_metric):\n",
    "    if flag == 'reverse': # Use vanilla algoritms for reverse-search feature scoring to ensure a consistent comparison\n",
    "        if clustering_algorithm == 'kmedoids':\n",
    "            return KMedoids(n_clusters=2, init='k-medoids++'), 0, 'na', 0, 0\n",
    "        elif clustering_algorithm == 'hdbscan':\n",
    "            return HDBSCAN(store_centers=\"medoid\"), 0, 'na', 0, 0\n",
    "    else:\n",
    "        if flag == 'common':\n",
    "            metric = common_metric\n",
    "        else:\n",
    "            metrics = ['manhattan', 'euclidean', 'chebyshev', 'canberra', 'hamming']  # Available metrics - pick one\n",
    "            metric = rng.choice(metrics)  # Randomly select a metric\n",
    "            \n",
    "        if clustering_algorithm == 'kmedoids':\n",
    "            #metrics = ['manhattan', 'euclidean', 'cosine']  # Available metrics - pick one\n",
    "            k_val = rng.integers(2, 10)\n",
    "            return KMedoids(n_clusters=k_val, init='k-medoids++', metric=metric), k_val, metric, 0, 0\n",
    "        elif clustering_algorithm == 'hdbscan':\n",
    "            min_cluster_size = rng.choice([10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150])\n",
    "            min_samples = min_cluster_size + rng.choice([10, 20, 30, 40, 50, 60, 70]) # Higher values force conservative clustering\n",
    "            return HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, metric=metric, cluster_selection_method='eom', store_centers=\"medoid\"), 0, metric, min_cluster_size, min_samples\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported clustering algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1413734a-5338-4517-891d-9447d159c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Primary function - Evaluates the subset_array\n",
    "def evaluate_subset(clustering_algorithm, subset_array, np_array, rng, flag, common_metric):\n",
    "    clustering_instance, k_val, metric, min_cluster_size, min_samples = initialize_clustering_instance(clustering_algorithm, rng, flag, common_metric)\n",
    "    current_labels = clustering_instance.fit_predict(subset_array)\n",
    "    clustering_medoids = getattr(clustering_instance, 'cluster_centers_', getattr(clustering_instance, 'medoids_', None))\n",
    "    return score_subset_clusters(subset_array, np_array, current_labels, clustering_medoids), k_val, metric, min_cluster_size, min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10669439-74f2-44c6-95b5-363e66a54f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Primary function - Creates a starter_set from the available_indices\n",
    "def select_starter_set(available_indices, interim_features, starter_set_size, rng):\n",
    "    remaining_indices = list(available_indices - interim_features)\n",
    "    if len(remaining_indices) <= starter_set_size: # Check if there are enough indices to create a full starter_set\n",
    "        return rng.shuffle(remaining_indices), 0\n",
    "    else:\n",
    "        return rng.choice(remaining_indices, starter_set_size, replace=False), 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f59bd6d8-3c95-4266-9c89-971c8ac692c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Primary function - Adds a new score and the clustering parameters to the collection\n",
    "def update_best_scores(best_scores, score, feature, k_val, metric, cluster_size, num_samples, interim_features, available_indices):\n",
    "    best_scores['k_val'].append(k_val) # K-Medoids - Track the target k\n",
    "    best_scores['cluster_size'].append(cluster_size) # HDBSCAN - Track the size of the clusters\n",
    "    best_scores['num_samples'].append(num_samples)  # HDBSCAN - Track the number of samples per cluster\n",
    "    best_scores['metric'].append(metric)  # K-Medoids - Track the metric\n",
    "    interim_features.add(feature)\n",
    "    available_indices.remove(feature)\n",
    "    return available_indices, interim_features, best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9399a35d-6a47-45b6-bedd-a25fbf9cd670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Primary function - Score the interim_features as they are removed - determine whether any ore not needed.\n",
    "def evaluate_feature_removal(np_array, clustering_algorithm, current_features, feature_to_remove, rng, flag, common_metric):\n",
    "    # Remove the specified feature from the current feature set\n",
    "    modified_features = [f for f in current_features if f != feature_to_remove]\n",
    "    \n",
    "    # Subset the np_array to include only the modified feature set\n",
    "    subset_array = np_array[:, modified_features]\n",
    "    \n",
    "    # Evaluate the clustering with the modified feature set\n",
    "    score, _, _, _, _, = evaluate_subset(clustering_algorithm, subset_array, np_array, rng, flag, common_metric)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efc0c840-19e5-4daf-ae50-ddab49cb3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Primary function - Evaluate the scored interim_features - remove unneeded if below criteria.\n",
    "def perform_reverse_search(np_array, clustering_algorithm, interim_features, global_best_score, rng):\n",
    "    refined_features = set(interim_features)\n",
    "    feature_removed = True\n",
    "\n",
    "    print(' --------------- Beginning Reverse-searching to refine the interim_features ... --------------- ')\n",
    "\n",
    "    while feature_removed:\n",
    "        if len(refined_features) == 5:\n",
    "            print(' --------------- Need to collect more candidate features - we have 5 features left.')\n",
    "            return refined_features, global_best_score\n",
    "        else:\n",
    "            test_scores = []  # Store feature removal scores for comparison\n",
    "    \n",
    "            # Evaluate feature removal in parallel\n",
    "            test_scores = Parallel(n_jobs=10)(\n",
    "                delayed(evaluate_feature_removal)(np_array, clustering_algorithm, list(refined_features), feature, rng, 'reverse', 'na')\n",
    "                for feature in refined_features\n",
    "            )\n",
    "            test_scores = list(zip(refined_features, test_scores))  # Combine feature indices with their scores\n",
    "    \n",
    "            candidate_removal, candidate_score = max(test_scores, key=lambda x: x[1])  # Use the key argument of the max() function to specify that we want to find the maximum based on the second element of each tuple in test_scores\n",
    "    \n",
    "            if candidate_score > global_best_score:  # The score improves with the feature removed and is better than the local best score\n",
    "                refined_features.remove(candidate_removal)\n",
    "                global_best_score = candidate_score  # Update the local best score\n",
    "                print(f' --------------- Feature {candidate_removal} was removed from the refined_features. Current removal score is {global_best_score:.4f}')\n",
    "                feature_removed = True\n",
    "            else:\n",
    "                print(' --------------- No feature met the removal criteria ')\n",
    "                feature_removed = False\n",
    "    \n",
    "    return refined_features, global_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d6976e6-2b0d-4c92-85f1-126305fd31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Primary function - Evaluate the available indices - iterate through the collection to identify those that improve scores by removal.\n",
    "def perform_reverse_index_removal(np_array, clustering_algorithm, available_indices, global_best_score, rng):\n",
    "    indices_removed = True  # Flag to track if any features were removed in the last pass\n",
    "\n",
    "    print(' --------------- Starting Reverse Index Removal --------------- ')\n",
    "    while indices_removed: # remove as many as possible for each run...\n",
    "        indices_removed = False  # Reset the flag for the current pass\n",
    "        if len(available_indices) == 10:\n",
    "            print(' --------------- No index removal possible.')\n",
    "            return available_indices, global_best_score\n",
    "        else:\n",
    "            test_scores = []  # Store feature removal scores for comparison\n",
    "\n",
    "            # Evaluate feature removal in parallel\n",
    "            test_scores = Parallel(n_jobs=10)(\n",
    "                delayed(evaluate_feature_removal)(np_array, clustering_algorithm, list(available_indices), feature, rng, 'reverse', 'na')\n",
    "                for feature in available_indices\n",
    "            )\n",
    "            test_scores = list(zip(available_indices, test_scores))  # Combine feature indices with their scores\n",
    "\n",
    "            candidate_removal, candidate_score = max(test_scores, key=lambda x: x[1])  # Use the key argument of the max() function to specify that we want to find the maximum based on the second element of each tuple in test_scores\n",
    "\n",
    "            if candidate_score > (global_best_score + 0.05):  # The score improves with the feature removed and is incrementally better than the global removal score\n",
    "                available_indices.remove(candidate_removal)\n",
    "                indices_removed = True  # Indicate that an index was removed in this pass\n",
    "                global_best_score = candidate_score  # Update the global removal score\n",
    "                print(f' --------------- Index {candidate_removal} was removed. The current removal score is {global_best_score:.4f}')\n",
    "\n",
    "    print(f' --------------- Reverse Index Removal completed ---- {len(available_indices)} available_indices remaining. ')\n",
    "    return available_indices, global_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bf2dcb5-d550-4c29-92c2-dd40109e6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper function - Shuffles the contents of the feature array during combination evaluation\n",
    "def shuffle_array(array, rng):\n",
    "    # Shuffle the array along the first axis (rows)\n",
    "    shuffled_array = array.copy()\n",
    "    rng.shuffle(shuffled_array, axis=0)\n",
    "    return shuffled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3e461c3-8236-48ae-a499-223a7d4caf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Primary function - Evaluates new features alongside the initial collection of important features\n",
    "def evaluate_combinations(np_array, clustering_algorithm, available_indices, refined_features, rng, best_scores, global_best_score):\n",
    "    iteration = 0\n",
    "\n",
    "    print(' *************** Begin combination evaluation ...  *************** ')\n",
    "\n",
    "    # Identify the most common measurement metric and use that for the combination evaluations\n",
    "    metric_counter = Counter(best_scores['metric'])\n",
    "    common_metric = metric_counter.most_common(1)[0][0]  # Return the most common metric directly\n",
    "    print(f' ***************  Most common metric: {common_metric}')\n",
    "        \n",
    "    while True:  # Simulate a do-while loop\n",
    "        iteration += 1\n",
    "\n",
    "        # Create a list of features to evaluate\n",
    "        features_to_evaluate = list(available_indices - refined_features)\n",
    "\n",
    "        # Evaluate combination and features in parallel\n",
    "        evaluation_results = Parallel(n_jobs=10)(\n",
    "            delayed(evaluate_subset)(clustering_algorithm, shuffle_array(np_array[:, list(refined_features) + [feature]], rng), np_array, rng, 'na', common_metric)\n",
    "            for feature in features_to_evaluate\n",
    "        )\n",
    "\n",
    "        for idx, (normalized_score, k_val, metric, cluster_size, num_samples) in enumerate(evaluation_results):\n",
    "            feature = features_to_evaluate[idx]\n",
    "\n",
    "            # Update best combination if necessary\n",
    "            if normalized_score > global_best_score:\n",
    "                global_best_score = normalized_score\n",
    "                best_feature = feature\n",
    "                best_clust = cluster_size\n",
    "                best_num_samps = num_samples\n",
    "                best_k = k_val\n",
    "                best_metric = metric\n",
    "                iteration = 0  # Reset the counter\n",
    "\n",
    "                # If a better combination was found, add its feature to refined_features\n",
    "                available_indices, refined_features, best_scores = update_best_scores(best_scores, global_best_score, best_feature, best_k, best_metric, best_clust, best_num_samps, refined_features, available_indices)\n",
    "                print(f' *************** Feature Number {best_feature} added to refined_features with score = {global_best_score:.4f} - {len(refined_features)} refined_features found so far...')\n",
    "\n",
    "        if iteration % 20 == 0:\n",
    "            print(f' *************** Combination Evaluation Iteration {iteration}')\n",
    "        if (iteration % 205 ==0): # Combination feature searching has stalled for some other reason - exit\n",
    "            break\n",
    "        \n",
    "    print(f' *************** Processing completed - Total number of identified features = {len(refined_features)}')\n",
    "\n",
    "    return available_indices, refined_features, best_scores, global_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c56d60e8-7cb7-4655-a449-1e6a6bada002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Primary function - Select candidate features from the available_indices via starter_sets\n",
    "def select_candidate_features_via_starter_set(np_array, clustering_algorithm, interim_features, available_indices, best_scores, global_best_score, big_score_jump, rng):\n",
    "    ss_flag = 1  # Initalize the local variable in case this function is entered and the processing is skipped\n",
    "    starter_set_size = 10\n",
    "    k_val, min_cluster_size, min_samples, cluster_size, num_samples, iteration = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    print(' +++++++++++++++ Begin selecting candidate features via starter_sets ... +++++++++++++++ ')\n",
    "         \n",
    "    while True:  # Simulate a do-while loop\n",
    "        starter_set, ss_flag = select_starter_set(available_indices, interim_features, starter_set_size, rng)\n",
    "\n",
    "        # Create a list of features to evaluate\n",
    "        features_to_evaluate = list(available_indices - interim_features - set(starter_set))\n",
    "\n",
    "        # Evaluate features in parallel\n",
    "        evaluation_results = Parallel(n_jobs=10)(\n",
    "            delayed(evaluate_subset)(clustering_algorithm, np_array[:, list(starter_set) + [feature]], np_array, rng, 'na', 'na')\n",
    "            for feature in features_to_evaluate\n",
    "        )\n",
    "\n",
    "        for idx, (normalized_score, k_val, metric, cluster_size, num_samples) in enumerate(evaluation_results):\n",
    "            feature = features_to_evaluate[idx]\n",
    "\n",
    "            # Update best feature if necessary\n",
    "            if normalized_score > global_best_score:\n",
    "                if (normalized_score - global_best_score) > 0.7:\n",
    "                    big_score_jump = True\n",
    "                    interim_features.update(starter_set)\n",
    "                    available_indices.difference_update(starter_set)\n",
    "                    print(' +*+*+*+*+*+*  Detected a big_score_jump - Added the entire starter_set to the interim_features *+*+*+*+*+*+')\n",
    "                global_best_score = normalized_score\n",
    "                best_new_feature = feature\n",
    "                best_clust = cluster_size\n",
    "                best_num_samps = num_samples\n",
    "                best_k = k_val\n",
    "                best_metric = metric\n",
    "                \n",
    "                available_indices, interim_features, best_scores = update_best_scores(best_scores, global_best_score, best_new_feature, best_k, best_metric, best_clust, best_num_samps, interim_features, available_indices)\n",
    "                print(f' +++++++++++++++ Found a new interim feature: {best_new_feature} - with score = {global_best_score:.4f} - {len(interim_features)} found so far...')\n",
    "                iteration = 0  # Reset the flag\n",
    "        \n",
    "        if ss_flag == 0:\n",
    "            print(' +++++++++++++++  Finished processing all available features ... ')\n",
    "            break\n",
    "        else:\n",
    "            iteration += 1\n",
    "            if iteration % 40 ==0:\n",
    "                print(f' +++++++++++++++ Starter_set Filling Iteration {iteration}')\n",
    "            if (iteration % 205 ==0): # Starter_set feature searching has stalled for some other reason - exit\n",
    "                print(f' +++++++++++++++ Exiting Starter_set Filling - {len(interim_features)} Interim Features found so far...')\n",
    "                break\n",
    "\n",
    "    return available_indices, interim_features, best_scores, global_best_score, ss_flag, big_score_jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afefe405-91a6-466e-825f-58af1652ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Main function - Orchestrates the analysis\n",
    "def optimal_feature_clusters(np_array, clustering_algorithm):\n",
    "    rng = default_rng()  # Use numpy's random number generation.\n",
    "    n_features = np_array.shape[1]\n",
    "    available_indices = set(range(n_features))\n",
    "    interim_features = set()\n",
    "    best_scores = {'k_val': [], 'cluster_size': [], 'num_samples': [], 'metric': []}\n",
    "    best_score = 0.2 # Starter score for first pass into select_candidate_features_via_starter_set\n",
    "    big_score_jump = False  #Prepare the flag for locating a very important feature\n",
    "\n",
    "    # Start the processing by looking for features that negatively impact scoring - remove them from the set (reducing set size increases processing speed).\n",
    "    print(' =============== Start processing ... ')\n",
    "    available_indices, global_best_score = perform_reverse_index_removal(np_array, clustering_algorithm, available_indices, best_score, rng)\n",
    "    available_indices, refined_features, best_scores, global_best_score, ss_flag, big_score_jump = select_candidate_features_via_starter_set(np_array, clustering_algorithm, interim_features, available_indices, best_scores, best_score, big_score_jump, rng)\n",
    "    updated = True\n",
    "    \n",
    "    while updated and (ss_flag == 1):\n",
    "        # Use the current candidates to locate any new important features\n",
    "        available_indices, refined_features, best_scores, global_best_score = evaluate_combinations(np_array, clustering_algorithm, available_indices, refined_features, rng, best_scores, global_best_score)\n",
    "\n",
    "        # Test the current candidates to remove any that appear to be restricting the scores\n",
    "        refined_features, global_best_score = perform_reverse_search(np_array, clustering_algorithm, refined_features, global_best_score, rng)\n",
    "        refined_features_after_reverse_search = len(refined_features)  # Collect the current number of refined features\n",
    "        \n",
    "        if big_score_jump:\n",
    "            # Try to remove any unimportant indices\n",
    "            available_indices, global_best_score = perform_reverse_index_removal(np_array, clustering_algorithm, available_indices, global_best_score, rng)\n",
    "            # Make a limited attempt to locate any more features with the starter sets\n",
    "            available_indices, refined_features, best_scores, global_best_score, ss_flag, global_removal_score, big_score_jump = select_candidate_features_via_starter_set(np_array, clustering_algorithm, refined_features, available_indices, best_scores, global_best_score, big_score_jump, rng)\n",
    "\n",
    "            refined_features, global_best_score = perform_reverse_search(np_array, clustering_algorithm, refined_features, global_best_score, rng)\n",
    "            \n",
    "            if len(refined_features) > refined_features_after_reverse_search:  # Test if we have collected more features through the processing\n",
    "                updated = True\n",
    "                print(' =============== Moving to next iteration... ') # Looks like we got past the big score jump\n",
    "                big_score_jump = False # Reset the flag\n",
    "                continue\n",
    "            else:\n",
    "                # Collect the current state of the candidates\n",
    "                current_state = len(refined_features)\n",
    "                # Try to work with the current candidates and explore combinations\n",
    "                available_indices, refined_features, best_scores, global_best_score = evaluate_combinations(np_array, clustering_algorithm, available_indices, refined_features, rng, best_scores, global_best_score)\n",
    "\n",
    "                if len(refined_features) > current_state:  # We made progress, so keep working...\n",
    "                    updated = True\n",
    "                    print(' =============== Moving to next iteration... ') # Looks like we got past the big score jump\n",
    "                    big_score_jump = False # Reset the flag\n",
    "                    continue\n",
    "                else:\n",
    "                    print(' =============== Processing completed. ') # Still blocked by the big score jump\n",
    "                    break\n",
    "        else: # Continue processing features\n",
    "            updated = False\n",
    "\n",
    "            # Identify any more of the available_indices that negatively affect scoring - remove them\n",
    "            available_indices, global_best_score = perform_reverse_index_removal(np_array, clustering_algorithm, available_indices, global_best_score, rng)\n",
    "\n",
    "            available_indices, refined_features, best_scores, global_best_score, ss_flag, big_score_jump = select_candidate_features_via_starter_set(np_array, clustering_algorithm, refined_features, available_indices, best_scores, global_best_score, big_score_jump, rng)\n",
    "\n",
    "            refined_features, global_best_score = perform_reverse_search(np_array, clustering_algorithm, refined_features, global_best_score, rng)\n",
    "            \n",
    "            available_indices, refined_features, best_scores, global_best_score = evaluate_combinations(np_array, clustering_algorithm, available_indices, refined_features, rng, best_scores, global_best_score)\n",
    "\n",
    "            refined_features, global_best_score = perform_reverse_search(np_array, clustering_algorithm, refined_features, global_best_score, rng)\n",
    "            \n",
    "            if len(refined_features) != refined_features_after_reverse_search:  # Test if the number of collected features has changed through the processing\n",
    "                updated = True\n",
    "                print(' =============== Moving to next iteration... ')\n",
    "            else:\n",
    "                print(' =============== Processing completed. ')\n",
    "\n",
    "    return best_scores['k_val'], best_scores['metric'], refined_features, best_scores['cluster_size'], best_scores['num_samples'],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b4c3a-fd1c-4330-8fc6-8bcce04431f8",
   "metadata": {},
   "source": [
    "### Perform PFA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34f8db-361e-41de-a411-4208e9890670",
   "metadata": {},
   "source": [
    "#### KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb6c4fd7-6304-4b22-bfa7-0ffb3657bb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =============== Start processing ... \n",
      " --------------- Starting Reverse Index Removal --------------- \n",
      " --------------- Index 43 was removed. The current removal score is 0.6297\n",
      " --------------- Reverse Index Removal completed ---- 49 available_indices remaining. \n",
      " +++++++++++++++ Begin selecting candidate features via starter_sets ... +++++++++++++++ \n",
      " +++++++++++++++ Found a new interim feature: 7 - with score = 0.3604 - 1 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 24 - with score = 0.3728 - 2 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 49 - with score = 0.4097 - 3 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 1 - with score = 0.9499 - 4 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 2 - with score = 0.9674 - 5 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 8 - with score = 0.9688 - 6 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 16 - with score = 1.0120 - 7 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 22 - with score = 1.1275 - 8 found so far...\n",
      " +++++++++++++++ Starter_set Filling Iteration 40\n",
      " +++++++++++++++ Starter_set Filling Iteration 80\n",
      " +++++++++++++++ Starter_set Filling Iteration 120\n",
      " +++++++++++++++ Found a new interim feature: 28 - with score = 1.1675 - 9 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 44 - with score = 1.1756 - 10 found so far...\n",
      " +++++++++++++++ Starter_set Filling Iteration 40\n",
      " +++++++++++++++ Starter_set Filling Iteration 80\n",
      " +++++++++++++++ Starter_set Filling Iteration 120\n",
      " +++++++++++++++ Starter_set Filling Iteration 160\n",
      " +++++++++++++++ Starter_set Filling Iteration 200\n",
      " +++++++++++++++ Exiting Starter_set Filling - 10 Interim Features found so far...\n",
      " *************** Begin combination evaluation ...  *************** \n",
      " ***************  Most common metric: euclidean\n",
      " *************** Combination Evaluation Iteration 20\n",
      " *************** Combination Evaluation Iteration 40\n",
      " *************** Combination Evaluation Iteration 60\n",
      " *************** Combination Evaluation Iteration 80\n",
      " *************** Combination Evaluation Iteration 100\n",
      " *************** Combination Evaluation Iteration 120\n",
      " *************** Combination Evaluation Iteration 140\n",
      " *************** Combination Evaluation Iteration 160\n",
      " *************** Combination Evaluation Iteration 180\n",
      " *************** Combination Evaluation Iteration 200\n",
      " *************** Processing completed - Total number of identified features = 10\n",
      " --------------- Beginning Reverse-searching to refine the interim_features ... --------------- \n",
      " --------------- No feature met the removal criteria \n",
      " --------------- Starting Reverse Index Removal --------------- \n",
      " --------------- Reverse Index Removal completed ---- 39 available_indices remaining. \n",
      " +++++++++++++++ Begin selecting candidate features via starter_sets ... +++++++++++++++ \n",
      " +++++++++++++++ Starter_set Filling Iteration 40\n",
      " +++++++++++++++ Starter_set Filling Iteration 80\n",
      " +++++++++++++++ Starter_set Filling Iteration 120\n",
      " +++++++++++++++ Starter_set Filling Iteration 160\n",
      " +++++++++++++++ Starter_set Filling Iteration 200\n",
      " +++++++++++++++ Exiting Starter_set Filling - 10 Interim Features found so far...\n",
      " --------------- Beginning Reverse-searching to refine the interim_features ... --------------- \n",
      " --------------- No feature met the removal criteria \n",
      " *************** Begin combination evaluation ...  *************** \n",
      " ***************  Most common metric: euclidean\n",
      " *************** Combination Evaluation Iteration 20\n",
      " *************** Combination Evaluation Iteration 40\n",
      " *************** Combination Evaluation Iteration 60\n",
      " *************** Combination Evaluation Iteration 80\n",
      " *************** Combination Evaluation Iteration 100\n",
      " *************** Combination Evaluation Iteration 120\n",
      " *************** Combination Evaluation Iteration 140\n",
      " *************** Combination Evaluation Iteration 160\n",
      " *************** Combination Evaluation Iteration 180\n",
      " *************** Combination Evaluation Iteration 200\n",
      " *************** Processing completed - Total number of identified features = 10\n",
      " --------------- Beginning Reverse-searching to refine the interim_features ... --------------- \n",
      " --------------- No feature met the removal criteria \n",
      " =============== Processing completed. \n",
      " ^^^ RUN #1 --- PFA KMedoids Clustering Execution in 363.0229 seconds ^^^ \n",
      " Best k values = [2, 2, 2, 2, 2, 2, 2, 2, 2, 4]\n",
      " Best metric values = ['chebyshev', 'euclidean', 'euclidean', 'euclidean', 'euclidean', 'euclidean', 'euclidean', 'hamming', 'hamming', 'hamming']\n",
      " best features = [1, 2, 7, 8, 16, 22, 24, 28, 44, 49]\n"
     ]
    }
   ],
   "source": [
    "# ^^^^^^^^^^^^^^^^^KMedoids Run 1 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "best_kmedoid_features_run1 = set()\n",
    "\n",
    "complete1_df = test_df.copy()\n",
    "complete1_np = complete1_df.to_numpy()\n",
    "\n",
    "# Run the experiment using the complete (non-pca) dataframe and identify the clustering algorithm by name.\n",
    "best_k_vals_run1, best_kmetric_run1, best_kmedoid_features_run1, na1, na2 = optimal_feature_clusters(complete1_np, 'kmedoids')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^ RUN #1 --- PFA KMedoids Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' Best k values = {sorted(best_k_vals_run1)}')\n",
    "print(f' Best metric values = {sorted(best_kmetric_run1)}')\n",
    "print(f' best features = {sorted(best_kmedoid_features_run1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3fb7e3-4a66-4647-acd5-f90e3f711a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^^^^^^^^^^^^^^^^KMedoids Run 2 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "best_kmedoid_features_run2 = set()\n",
    "\n",
    "complete2_df = test_df.copy()\n",
    "complete2_np = complete2_df.to_numpy()\n",
    "\n",
    "# Run the experiment using the complete (non-pca) dataframe and identify the clustering algorithm by name.\n",
    "best_k_vals_run2, best_kmetric_run2, best_kmedoid_features_run2, na1, na2 = optimal_feature_clusters(complete2_np, 'kmedoids')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^ RUN #2 --- PFA KMedoids Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' Best k values = {sorted(best_k_vals_run2)}')\n",
    "print(f' Best metric values = {sorted(best_kmetric_run2)}')\n",
    "print(f' best features = {sorted(best_kmedoid_features_run2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07fa40-b99c-4065-8890-922cbc609839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^^^^^^^^^^^^^^^^KMedoids Run 3 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "best_kmedoid_features_run3 = set()\n",
    "\n",
    "complete3_df = test_df.copy()\n",
    "complete3_np = complete3_df.to_numpy()\n",
    "\n",
    "# Run the experiment using the complete (non-pca) dataframe and identify the clustering algorithm by name.\n",
    "best_k_vals_run3, best_kmetric_run3, best_kmedoid_features_run3, na1, na2 = optimal_feature_clusters(complete3_np, 'kmedoids')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^ RUN #3 --- PFA KMedoids Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' Best k values = {sorted(best_k_vals_run3)}')\n",
    "print(f' Best metric values = {sorted(best_kmetric_run3)}')\n",
    "print(f' best features = {sorted(best_kmedoid_features_run3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea2f60-8691-422b-823f-87a447ce3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^^^^^^^^^^^^^^^^KMedoids Run 4 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "best_kmedoid_features_run4 = set()\n",
    "\n",
    "complete4_df = test_df.copy()\n",
    "complete4_np = complete4_df.to_numpy()\n",
    "\n",
    "# Run the experiment using the complete (non-pca) dataframe and identify the clustering algorithm by name.\n",
    "best_k_vals_run4, best_kmetric_run4, best_kmedoid_features_run4, na1, na2 = optimal_feature_clusters(complete4_np, 'kmedoids')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^ RUN #4 --- PFA KMedoids Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' Best k values = {sorted(best_k_vals_run4)}')\n",
    "print(f' Best metric values = {sorted(best_kmetric_run4)}')\n",
    "print(f' best features = {sorted(best_kmedoid_features_run4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97eb4a-afa0-4f9f-8e0e-ee6abb8cb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^^^^^^^^^^^^^^^^KMedoids Run 5 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "best_kmedoid_features_run5 = set()\n",
    "\n",
    "complete5_df = test_df.copy()\n",
    "complete5_np = complete5_df.to_numpy()\n",
    "\n",
    "# Run the experiment using the complete (non-pca) dataframe and identify the clustering algorithm by name.\n",
    "best_k_vals_run5, best_kmetric_run5, best_kmedoid_features_run5, na1, na2 = optimal_feature_clusters(complete5_np, 'kmedoids')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^ RUN #5 --- PFA KMedoids Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' Best k values = {sorted(best_k_vals_run5)}')\n",
    "print(f' Best metric values = {sorted(best_kmetric_run5)}')\n",
    "print(f' best features = {sorted(best_kmedoid_features_run5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae22b35-8d36-4c24-a96f-a1aab320dfae",
   "metadata": {},
   "source": [
    "### Process the collected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23bc1c-91b9-46a3-b0c0-ae2aa02441e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify common k-value and features in the KMedoids output\n",
    "# Combine the run results\n",
    "all_k_values = best_k_vals_run1 + best_k_vals_run2 + best_k_vals_run3 + best_k_vals_run4 + best_k_vals_run5\n",
    "all_kmetric_values = best_kmetric_run1 + best_kmetric_run2 + best_kmetric_run3 + best_kmetric_run4 + best_kmetric_run5\n",
    "all_features = [best_kmedoid_features_run1, best_kmedoid_features_run2, best_kmedoid_features_run3, best_kmedoid_features_run4, best_kmedoid_features_run5]\n",
    "\n",
    "# Count the frequency of the values and pick the maximum in the event of a tie\n",
    "k_counter = Counter(all_k_values)\n",
    "most_common_k_values = k_counter.most_common()  # This gives a list of (k, count) pairs\n",
    "max_count = most_common_k_values[0][1]  # The count of the most frequent k\n",
    "# Filter for ties: get all k values with the count equal to max_count\n",
    "ties = [k for k, count in most_common_k_values if count == max_count]\n",
    "# Select the largest k from the ties\n",
    "selected_k = max(ties)  # Select the largest k from the ties\n",
    "\n",
    "kmetric_counter = Counter(all_kmetric_values)\n",
    "most_common_kmetric = kmetric_counter.most_common(1)[0][0]  # Return the most common metric directly\n",
    "print(f'Most common metric: {most_common_kmetric}')\n",
    "\n",
    "# For features, use set operations to find common and all selected features\n",
    "final_kmedoid_common_features = set.intersection(*all_features)\n",
    "final_kmedoid_combined_features = set.union(*all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18e7136-bd8f-464d-a3fb-5e96f4d111ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(final_kmedoid_common_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659dadb-42cf-45c4-a247-a3e08e27698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(final_kmedoid_combined_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f7f9b-2888-4f44-8c4b-178e6558b51f",
   "metadata": {},
   "source": [
    "### Perform clustering with the reduced feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d1d8b-dacc-43f1-bfc8-3c506dfa0ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the selected features for the final KMedoids clustering\n",
    "\n",
    "# Add a test for whether there are any common results\n",
    "if (final_kmedoid_common_features == set()) or (len(final_kmedoid_common_features) <= 5): # no common features OR not enough for processing\n",
    "    print(' No common features - skipping ahead... ')\n",
    "else:\n",
    "    # Create a DataFrame for the best common features\n",
    "    kmedoids_common_df = test_df.copy()\n",
    "    kmedoids_reduced_common_features_df = kmedoids_common_df.iloc[:,sorted(final_kmedoid_common_features)]\n",
    "    \n",
    "    # Perform clustering on the final set of common features with the common k-value\n",
    "    kmedoids_final_common_model = KMedoids(n_clusters=selected_k, init='k-medoids++', metric=most_common_kmetric, random_state=42)\n",
    "    kmedoids_final_common_labels = kmedoids_final_common_model.fit_predict(kmedoids_reduced_common_features_df)\n",
    "    kmedoids_final_common_cluster_centers = kmedoids_final_common_model.cluster_centers_\n",
    "    \n",
    "    # Create the dataframes for visualization\n",
    "    kmedoids_final_viz_common_features_df = test_df.copy()\n",
    "    kmedoids_final_reduced_common_features_df = kmedoids_final_viz_common_features_df.iloc[:,sorted(final_kmedoid_common_features)]\n",
    "    kmedoids_final_reduced_common_features_df['KMedoids Clusters'] = kmedoids_final_common_labels\n",
    "    \n",
    "    kmedoids_final_COMMON_complete_features_df = test_df.copy()\n",
    "    kmedoids_final_COMMON_complete_features_df['KMedoids Clusters'] = kmedoids_final_common_labels\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Create a DataFrame for the best combined features\n",
    "kmedoids_combined_df = test_df.copy()\n",
    "kmedoids_reduced_combined_features_df = kmedoids_combined_df.iloc[:,sorted(final_kmedoid_combined_features)]\n",
    "\n",
    "# Perform clustering on the final set of combined features with the common k-value\n",
    "kmedoids_final_combined_model = KMedoids(n_clusters=selected_k, init='k-medoids++', metric=most_common_kmetric, random_state=42)\n",
    "kmedoids_final_combined_labels = kmedoids_final_combined_model.fit_predict(kmedoids_reduced_combined_features_df)\n",
    "kmedoids_final_combined_cluster_centers = kmedoids_final_combined_model.cluster_centers_\n",
    "\n",
    "# Create the dataframes for visualization\n",
    "kmedoids_final_viz_combined_features_df = test_df.copy()\n",
    "kmedoids_final_reduced_combined_features_df = kmedoids_final_viz_combined_features_df.iloc[:,sorted(final_kmedoid_combined_features)]\n",
    "kmedoids_final_reduced_combined_features_df['KMedoids Clusters'] = kmedoids_final_combined_labels\n",
    "\n",
    "kmedoids_final_COMBINED_complete_features_df = test_df.copy()\n",
    "kmedoids_final_COMBINED_complete_features_df['KMedoids Clusters'] = kmedoids_final_combined_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9301026-e8cc-4797-a38e-e902ee6e6da7",
   "metadata": {},
   "source": [
    "### Generate final K-Medoids Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb71ca-9878-47da-a35d-4a93d4c60f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that generates a profile report and saves it to a file\n",
    "def generate_report(df, config_file, output_file):\n",
    "    profile = ProfileReport(df, config_file=config_file)\n",
    "    profile.to_file(output_file)\n",
    "    print(f\"Report {output_file} generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8962f17-85de-427a-9c83-9a1ff4b6f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Create YData reports to explore the KMedoids feature relationships\n",
    "# DataFrames and configuration for the reports\n",
    "\n",
    "# Add a test for whether there are any common results\n",
    "if (final_kmedoid_common_features == set()) or (len(final_kmedoid_common_features) <= 5): # no common features found\n",
    "    print(' No common features')\n",
    "    now = str(time.time_ns()) # Create a timestamp for unique filename sets\n",
    "    reports_info = [\n",
    "        {\n",
    "            'df': kmedoids_final_reduced_combined_features_df,\n",
    "            'config_file': 'config_ELR.yml',\n",
    "            'output_file': 'KMedoids_Final_REDUCED_COMBINED-Features_Report-' + now + '.html'\n",
    "        },\n",
    "        {\n",
    "            'df': kmedoids_final_COMBINED_complete_features_df,\n",
    "            'config_file': 'config_ELR.yml',\n",
    "            'output_file': 'KMedoids_Final_COMPLETE_COMBINED-Features_Report-' + now + '.html'\n",
    "        }\n",
    "    ]\n",
    "else:\n",
    "    now = str(time.time_ns()) # Create a timestamp for unique filename sets\n",
    "    reports_info = [\n",
    "        {\n",
    "            'df': kmedoids_final_reduced_common_features_df,\n",
    "            'config_file': 'config_ELR.yml',\n",
    "            'output_file': 'KMedoids_Final_REDUCED_COMMON-Features_Report-' + now + '.html'\n",
    "        },\n",
    "        {\n",
    "            'df': kmedoids_final_COMMON_complete_features_df,\n",
    "            'config_file': 'config_ELR.yml',\n",
    "            'output_file': 'KMedoids_Final_COMPLETE_COMMON-Features_Report-' + now + '.html'\n",
    "        },\n",
    "        {\n",
    "            'df': kmedoids_final_reduced_combined_features_df,\n",
    "            'config_file': 'config_ELR.yml',\n",
    "            'output_file': 'KMedoids_Final_REDUCED_COMBINED-Features_Report-' + now + '.html'\n",
    "        },\n",
    "        {\n",
    "            'df': kmedoids_final_COMBINED_complete_features_df,\n",
    "            'config_file': 'config_ELR.yml',\n",
    "            'output_file': 'KMedoids_Final_COMPLETE_COMBINED-Features_Report-' + now + '.html'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Use joblib to run the report generations in parallel\n",
    "# n_jobs=-1 uses all available CPUs\n",
    "Parallel(n_jobs=4)(delayed(generate_report)(\n",
    "    info['df'], info['config_file'], info['output_file']) for info in reports_info)\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^ Final KMedoids Clustering Report building in {stop - start:0.4f} seconds ^^^ ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c11ed-9958-4afe-92ec-19c4ee96efe3",
   "metadata": {},
   "source": [
    "#### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d14c35a-530d-4b64-b522-d62b091f3dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =============== Start processing ... \n",
      " --------------- Starting Reverse Index Removal --------------- \n",
      " --------------- Reverse Index Removal completed ---- 50 available_indices remaining. \n",
      " +++++++++++++++ Begin selecting candidate features via starter_sets ... +++++++++++++++ \n",
      " +++++++++++++++ Found a new interim feature: 0 - with score = 0.4716 - 1 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 7 - with score = 0.4816 - 2 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 13 - with score = 0.5194 - 3 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 16 - with score = 0.5460 - 4 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 3 - with score = 0.5482 - 5 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 6 - with score = 0.5782 - 6 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 48 - with score = 0.5942 - 7 found so far...\n",
      " +++++++++++++++ Starter_set Filling Iteration 40\n",
      " +++++++++++++++ Starter_set Filling Iteration 80\n",
      " +++++++++++++++ Starter_set Filling Iteration 120\n",
      " +++++++++++++++ Starter_set Filling Iteration 160\n",
      " +++++++++++++++ Starter_set Filling Iteration 200\n",
      " +++++++++++++++ Exiting Starter_set Filling - 7 Interim Features found so far...\n",
      " *************** Begin combination evaluation ...  *************** \n",
      " ***************  Most common metric: canberra\n",
      " *************** Feature Number 22 added to refined_features with score = 0.6412 - 8 refined_features found so far...\n",
      " *************** Combination Evaluation Iteration 0\n",
      " *************** Processing completed - Total number of identified features = 8\n",
      " --------------- Beginning Reverse-searching to refine the interim_features ... --------------- \n",
      " --------------- No feature met the removal criteria \n",
      " --------------- Starting Reverse Index Removal --------------- \n",
      " --------------- Reverse Index Removal completed ---- 42 available_indices remaining. \n",
      " +++++++++++++++ Begin selecting candidate features via starter_sets ... +++++++++++++++ \n",
      " +++++++++++++++ Starter_set Filling Iteration 40\n",
      " +++++++++++++++ Starter_set Filling Iteration 80\n",
      " +++++++++++++++ Starter_set Filling Iteration 120\n",
      " +++++++++++++++ Starter_set Filling Iteration 160\n",
      " +++++++++++++++ Starter_set Filling Iteration 200\n",
      " +++++++++++++++ Exiting Starter_set Filling - 8 Interim Features found so far...\n",
      " *************** Begin combination evaluation ...  *************** \n",
      " ***************  Most common metric: canberra\n",
      " *************** Feature Number 4 added to refined_features with score = 0.6584 - 9 refined_features found so far...\n",
      " *************** Combination Evaluation Iteration 0\n",
      " *************** Processing completed - Total number of identified features = 9\n",
      " =============== Moving to next iteration... \n",
      " *************** Begin combination evaluation ...  *************** \n",
      " ***************  Most common metric: canberra\n",
      " *************** Combination Evaluation Iteration 20\n",
      " *************** Combination Evaluation Iteration 40\n",
      " *************** Feature Number 26 added to refined_features with score = 0.6587 - 10 refined_features found so far...\n",
      " *************** Combination Evaluation Iteration 0\n",
      " *************** Processing completed - Total number of identified features = 10\n",
      " --------------- Beginning Reverse-searching to refine the interim_features ... --------------- \n",
      " --------------- No feature met the removal criteria \n",
      " --------------- Starting Reverse Index Removal --------------- \n",
      " --------------- Reverse Index Removal completed ---- 40 available_indices remaining. \n",
      " +++++++++++++++ Begin selecting candidate features via starter_sets ... +++++++++++++++ \n",
      " +++++++++++++++ Found a new interim feature: 2 - with score = 0.7087 - 11 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 43 - with score = 0.7452 - 12 found so far...\n",
      " +++++++++++++++ Starter_set Filling Iteration 40\n",
      " +++++++++++++++ Starter_set Filling Iteration 80\n",
      " +++++++++++++++ Starter_set Filling Iteration 120\n",
      " +++++++++++++++ Starter_set Filling Iteration 160\n",
      " +++++++++++++++ Starter_set Filling Iteration 200\n",
      " +++++++++++++++ Exiting Starter_set Filling - 12 Interim Features found so far...\n",
      " *************** Begin combination evaluation ...  *************** \n",
      " ***************  Most common metric: canberra\n",
      " *************** Combination Evaluation Iteration 20\n",
      " *************** Combination Evaluation Iteration 40\n",
      " *************** Combination Evaluation Iteration 60\n",
      " *************** Combination Evaluation Iteration 80\n",
      " *************** Combination Evaluation Iteration 100\n",
      " *************** Combination Evaluation Iteration 120\n",
      " *************** Combination Evaluation Iteration 140\n",
      " *************** Combination Evaluation Iteration 160\n",
      " *************** Combination Evaluation Iteration 180\n",
      " *************** Combination Evaluation Iteration 200\n",
      " *************** Processing completed - Total number of identified features = 12\n",
      " =============== Moving to next iteration... \n",
      " *************** Begin combination evaluation ...  *************** \n",
      " ***************  Most common metric: canberra\n",
      " *************** Combination Evaluation Iteration 20\n",
      " *************** Combination Evaluation Iteration 40\n",
      " *************** Combination Evaluation Iteration 60\n",
      " *************** Combination Evaluation Iteration 80\n",
      " *************** Combination Evaluation Iteration 100\n",
      " *************** Combination Evaluation Iteration 120\n",
      " *************** Combination Evaluation Iteration 140\n",
      " *************** Combination Evaluation Iteration 160\n",
      " *************** Combination Evaluation Iteration 180\n",
      " *************** Combination Evaluation Iteration 200\n",
      " *************** Processing completed - Total number of identified features = 12\n",
      " --------------- Beginning Reverse-searching to refine the interim_features ... --------------- \n",
      " --------------- No feature met the removal criteria \n",
      " --------------- Starting Reverse Index Removal --------------- \n",
      " --------------- Reverse Index Removal completed ---- 38 available_indices remaining. \n",
      " +++++++++++++++ Begin selecting candidate features via starter_sets ... +++++++++++++++ \n",
      " +++++++++++++++ Starter_set Filling Iteration 40\n",
      " +++++++++++++++ Starter_set Filling Iteration 80\n",
      " +++++++++++++++ Starter_set Filling Iteration 120\n",
      " +++++++++++++++ Starter_set Filling Iteration 160\n",
      " +++++++++++++++ Starter_set Filling Iteration 200\n",
      " +++++++++++++++ Exiting Starter_set Filling - 12 Interim Features found so far...\n",
      " *************** Begin combination evaluation ...  *************** \n",
      " ***************  Most common metric: canberra\n",
      " *************** Combination Evaluation Iteration 20\n",
      " *************** Combination Evaluation Iteration 40\n",
      " *************** Combination Evaluation Iteration 60\n",
      " *************** Combination Evaluation Iteration 80\n",
      " *************** Combination Evaluation Iteration 100\n",
      " *************** Combination Evaluation Iteration 120\n",
      " *************** Combination Evaluation Iteration 140\n",
      " *************** Combination Evaluation Iteration 160\n",
      " *************** Combination Evaluation Iteration 180\n",
      " *************** Combination Evaluation Iteration 200\n",
      " *************** Processing completed - Total number of identified features = 12\n",
      " =============== Processing completed. \n",
      " ^^^RUN #1 --- PFA HDBSCAN Clustering Execution in 562.1641 seconds ^^^ \n",
      " Best metric values = ['canberra', 'canberra', 'canberra', 'canberra', 'canberra', 'canberra', 'canberra', 'canberra', 'canberra', 'canberra', 'canberra', 'canberra']\n",
      " best features 1 = [0, 2, 3, 4, 6, 7, 13, 16, 22, 26, 43, 48]\n"
     ]
    }
   ],
   "source": [
    "# ^^^^^^^^^^^^^^^^^HDBSCAN Run 1 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "complete11_df = test_df.copy()\n",
    "complete11_np = complete11_df.to_numpy()\n",
    "\n",
    "best_hdbscan_features_run1 = set()\n",
    "\n",
    "# Run the experiment using the complete (non-pca) dataframe\n",
    "na1, best_hmetric_run1, best_hdbscan_features_run1, best_cluster_size_run1, best_num_samples_run1 = optimal_feature_clusters(complete11_np, 'hdbscan')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^RUN #1 --- PFA HDBSCAN Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' Best metric values = {sorted(best_hmetric_run1)}')\n",
    "print(f' best features 1 = {sorted(best_hdbscan_features_run1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c0cbc11-897d-4960-a7eb-7da8db806d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =============== Start processing ... \n",
      " --------------- Starting Reverse Index Removal --------------- \n",
      " --------------- Reverse Index Removal completed ---- 50 available_indices remaining. \n",
      " +++++++++++++++ Begin selecting candidate features via starter_sets ... +++++++++++++++ \n",
      " +++++++++++++++ Found a new interim feature: 46 - with score = 0.3617 - 1 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 0 - with score = 0.4761 - 2 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 1 - with score = 0.4919 - 3 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 5 - with score = 0.5253 - 4 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 22 - with score = 0.5900 - 5 found so far...\n",
      " +++++++++++++++ Starter_set Filling Iteration 40\n",
      " +++++++++++++++ Starter_set Filling Iteration 80\n",
      " +++++++++++++++ Found a new interim feature: 31 - with score = 0.6126 - 6 found so far...\n",
      " +++++++++++++++ Starter_set Filling Iteration 40\n",
      " +++++++++++++++ Starter_set Filling Iteration 80\n",
      " +++++++++++++++ Starter_set Filling Iteration 120\n",
      " +++++++++++++++ Starter_set Filling Iteration 160\n",
      " +++++++++++++++ Starter_set Filling Iteration 200\n",
      " +++++++++++++++ Exiting Starter_set Filling - 6 Interim Features found so far...\n",
      " *************** Begin combination evaluation ...  *************** \n",
      " ***************  Most common metric: canberra\n",
      " *************** Combination Evaluation Iteration 20\n",
      " *************** Combination Evaluation Iteration 40\n",
      " *************** Feature Number 26 added to refined_features with score = 0.6302 - 7 refined_features found so far...\n",
      " *************** Combination Evaluation Iteration 0\n",
      " *************** Processing completed - Total number of identified features = 7\n",
      " --------------- Beginning Reverse-searching to refine the interim_features ... --------------- \n",
      " --------------- No feature met the removal criteria \n",
      " --------------- Starting Reverse Index Removal --------------- \n",
      " --------------- Reverse Index Removal completed ---- 43 available_indices remaining. \n",
      " +++++++++++++++ Begin selecting candidate features via starter_sets ... +++++++++++++++ \n",
      " +++++++++++++++ Found a new interim feature: 24 - with score = 0.6309 - 8 found so far...\n",
      " +++++++++++++++ Starter_set Filling Iteration 40\n",
      " +++++++++++++++ Starter_set Filling Iteration 80\n",
      " +++++++++++++++ Starter_set Filling Iteration 120\n",
      " +++++++++++++++ Found a new interim feature: 36 - with score = 0.6357 - 9 found so far...\n",
      " +++++++++++++++ Found a new interim feature: 37 - with score = 0.6376 - 10 found so far...\n",
      " +++++++++++++++ Starter_set Filling Iteration 40\n",
      " +++++++++++++++ Starter_set Filling Iteration 80\n",
      " +++++++++++++++ Starter_set Filling Iteration 120\n",
      " +++++++++++++++ Starter_set Filling Iteration 160\n",
      " +++++++++++++++ Starter_set Filling Iteration 200\n",
      " +++++++++++++++ Exiting Starter_set Filling - 10 Interim Features found so far...\n",
      " --------------- Beginning Reverse-searching to refine the interim_features ... --------------- \n",
      " --------------- No feature met the removal criteria \n",
      " *************** Begin combination evaluation ...  *************** \n",
      " ***************  Most common metric: canberra\n",
      " *************** Combination Evaluation Iteration 20\n",
      " *************** Combination Evaluation Iteration 40\n",
      " *************** Combination Evaluation Iteration 60\n",
      " *************** Combination Evaluation Iteration 80\n",
      " *************** Combination Evaluation Iteration 100\n",
      " *************** Combination Evaluation Iteration 120\n",
      " *************** Combination Evaluation Iteration 140\n",
      " *************** Combination Evaluation Iteration 160\n",
      " *************** Combination Evaluation Iteration 180\n",
      " *************** Combination Evaluation Iteration 200\n",
      " *************** Processing completed - Total number of identified features = 10\n",
      " --------------- Beginning Reverse-searching to refine the interim_features ... --------------- \n",
      " --------------- No feature met the removal criteria \n",
      " =============== Moving to next iteration... \n",
      " *************** Begin combination evaluation ...  *************** \n",
      " ***************  Most common metric: canberra\n",
      " *************** Combination Evaluation Iteration 20\n",
      " *************** Combination Evaluation Iteration 40\n",
      " *************** Combination Evaluation Iteration 60\n",
      " *************** Combination Evaluation Iteration 80\n",
      " *************** Combination Evaluation Iteration 100\n",
      " *************** Combination Evaluation Iteration 120\n",
      " *************** Combination Evaluation Iteration 140\n",
      " *************** Combination Evaluation Iteration 160\n",
      " *************** Combination Evaluation Iteration 180\n",
      " *************** Combination Evaluation Iteration 200\n",
      " *************** Processing completed - Total number of identified features = 10\n",
      " --------------- Beginning Reverse-searching to refine the interim_features ... --------------- \n",
      " --------------- No feature met the removal criteria \n",
      " --------------- Starting Reverse Index Removal --------------- \n",
      " --------------- Reverse Index Removal completed ---- 40 available_indices remaining. \n",
      " +++++++++++++++ Begin selecting candidate features via starter_sets ... +++++++++++++++ \n",
      " +++++++++++++++ Starter_set Filling Iteration 40\n",
      " +++++++++++++++ Starter_set Filling Iteration 80\n",
      " +++++++++++++++ Starter_set Filling Iteration 120\n",
      " +++++++++++++++ Starter_set Filling Iteration 160\n",
      " +++++++++++++++ Starter_set Filling Iteration 200\n",
      " +++++++++++++++ Exiting Starter_set Filling - 10 Interim Features found so far...\n",
      " --------------- Beginning Reverse-searching to refine the interim_features ... --------------- \n",
      " --------------- No feature met the removal criteria \n",
      " *************** Begin combination evaluation ...  *************** \n",
      " ***************  Most common metric: canberra\n",
      " *************** Combination Evaluation Iteration 20\n",
      " *************** Combination Evaluation Iteration 40\n",
      " *************** Combination Evaluation Iteration 60\n",
      " *************** Combination Evaluation Iteration 80\n",
      " *************** Combination Evaluation Iteration 100\n",
      " *************** Combination Evaluation Iteration 120\n",
      " *************** Combination Evaluation Iteration 140\n",
      " *************** Combination Evaluation Iteration 160\n",
      " *************** Combination Evaluation Iteration 180\n",
      " *************** Combination Evaluation Iteration 200\n",
      " *************** Processing completed - Total number of identified features = 10\n",
      " --------------- Beginning Reverse-searching to refine the interim_features ... --------------- \n",
      " --------------- No feature met the removal criteria \n",
      " =============== Processing completed. \n",
      " ^^^RUN #2 --- PFA HDBSCAN Clustering Execution in 582.2426 seconds ^^^ \n",
      " Best metric values = ['canberra', 'canberra', 'canberra', 'canberra', 'canberra', 'canberra', 'canberra', 'canberra', 'canberra', 'canberra']\n",
      " best features 2 = [0, 1, 5, 22, 24, 26, 31, 36, 37, 46]\n"
     ]
    }
   ],
   "source": [
    "# ^^^^^^^^^^^^^^^^^HDBSCAN Run 2 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "complete12_df = test_df.copy()\n",
    "complete12_np = complete12_df.to_numpy()\n",
    "\n",
    "best_hdbscan_features_run2 = set()\n",
    "\n",
    "# Run the experiment using the complete (non-pca) dataframe\n",
    "na1, best_hmetric_run2, best_hdbscan_features_run2, best_cluster_size_run2, best_num_samples_run2 = optimal_feature_clusters(complete12_np, 'hdbscan')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^RUN #2 --- PFA HDBSCAN Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' Best metric values = {sorted(best_hmetric_run2)}')\n",
    "print(f' best features 2 = {sorted(best_hdbscan_features_run2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea4ff63-db44-4438-a75f-40cb769f5de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^^^^^^^^^^^^^^^^HDBSCAN Run 3 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "complete13_df = test_df.copy()\n",
    "complete13_np = complete13_df.to_numpy()\n",
    "\n",
    "best_hdbscan_features_run3 = set()\n",
    "\n",
    "# Run the experiment using the complete (non-pca) dataframe\n",
    "na1, best_hmetric_run3, best_hdbscan_features_run3, best_cluster_size_run3, best_num_samples_run3 = optimal_feature_clusters(complete13_np, 'hdbscan')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^RUN #3 --- PFA HDBSCAN Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' Best metric values = {sorted(best_hmetric_run3)}')\n",
    "print(f' best features 3  = {sorted(best_hdbscan_features_run3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df34786-a3b6-4707-bf72-7e6ac495226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^^^^^^^^^^^^^^^^HDBSCAN Run 4 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "complete14_df = test_df.copy()\n",
    "complete14_np = complete14_df.to_numpy()\n",
    "\n",
    "best_hdbscan_features_run4 = set()\n",
    "\n",
    "# Run the experiment using the complete (non-pca) dataframe\n",
    "na1, best_hmetric_run4, best_hdbscan_features_run4, best_cluster_size_run4, best_num_samples_run4 = optimal_feature_clusters(complete14_np, 'hdbscan')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^RUN #4 --- PFA HDBSCAN Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' Best metric values = {sorted(best_hmetric_run4)}')\n",
    "print(f' best features 4  = {sorted(best_hdbscan_features_run4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba08b14-f94c-46f8-91c0-02948eae7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^^^^^^^^^^^^^^^^HDBSCAN Run 5 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "complete15_df = test_df.copy()\n",
    "complete15_np = complete15_df.to_numpy()\n",
    "\n",
    "best_hdbscan_features_run5 = set()\n",
    "\n",
    "# Run the experiment using the complete (non-pca) dataframe\n",
    "na1, best_hmetric_run5, best_hdbscan_features_run5, best_cluster_size_run5, best_num_samples_run5 = optimal_feature_clusters(complete15_np, 'hdbscan')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^RUN #5 --- PFA HDBSCAN Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' Best metric values = {sorted(best_hmetric_run5)}')\n",
    "print(f' best features 5  = {sorted(best_hdbscan_features_run5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aca15f-803b-421e-adbf-34a91db94544",
   "metadata": {},
   "source": [
    "### Process the collected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8177fd-08e4-48f2-af27-ad5be59b89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify common cluster_size value and features in the HDBSCAN output\n",
    "# Combine the run results\n",
    "all_hmetric_values = best_hmetric_run1 + best_hmetric_run2 + best_hmetric_run3 + best_hmetric_run4 + best_hmetric_run5\n",
    "all_cluster_sizes = best_cluster_size_run1 + best_cluster_size_run2 + best_cluster_size_run3 + best_cluster_size_run4 + best_cluster_size_run5\n",
    "all_num_samples = best_num_samples_run1 + best_num_samples_run2 + best_num_samples_run3 + best_num_samples_run4 + best_num_samples_run5\n",
    "all_features = [best_hdbscan_features_run1, best_hdbscan_features_run2, best_hdbscan_features_run3, best_hdbscan_features_run4, best_hdbscan_features_run5]\n",
    "\n",
    "# Count the frequency of the metric values and pick the most common\n",
    "hmetric_counter = Counter(all_hmetric_values)\n",
    "most_common_hmetric = hmetric_counter.most_common(1)[0][0]  # Return the most common metric directly\n",
    "print(f'Most common metric: {most_common_hmetric}')\n",
    "\n",
    "# Count the frequency of the values and pick the maximum in the event of a tie\n",
    "cluster_size_counter = Counter(all_cluster_sizes)\n",
    "most_common_cluster_size_vals = cluster_size_counter.most_common()    # This gives a list of (cluster_size, count) pairs\n",
    "max_count_c_s = most_common_cluster_size_vals[0][1]  # The count of the most frequent cluster_size\n",
    "cluster_ties = [c_size for c_size, count in most_common_cluster_size_vals if count == max_count_c_s]\n",
    "selected_cluster_size = max(cluster_ties) # We want the largest cluster_size in case of a tie\n",
    "\n",
    "# Count the frequency of the values and pick the maximum in the event of a tie\n",
    "sample_size_counter = Counter(all_num_samples)\n",
    "most_common_sample_size_val = sample_size_counter.most_common()   # This gives a list of (sample_size, count) pairs\n",
    "max_count_sample_size = most_common_sample_size_val[0][1]  # The count of the most frequent sample_size\n",
    "sample_ties = [s_num for s_num, count in most_common_sample_size_val if count == max_count_sample_size]\n",
    "selected_sample_size = max(sample_ties) # We want the largest number of samples (per cluster) in the case of a tie\n",
    "\n",
    "# For features, use set operations to find common and combined sets of selected features\n",
    "final_hdbscan_common_features = set.intersection(*all_features)\n",
    "final_hdbscan_combined_features = set.union(*all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0cdfa-8707-4f86-a4d3-a14bef6e2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Final HDBSCAN common features = {final_hdbscan_common_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ad00d-3742-45c0-80e2-16e3b2adf076",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Final HDBSCAN combined features = {sorted(final_hdbscan_combined_features)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf250ef-ac83-42dc-bd5f-9325474632c5",
   "metadata": {},
   "source": [
    "### Perform clustering with the reduced feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6ca56-6246-4929-af4d-3c8e3b2c4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a test for whether there are any common results\n",
    "if (final_hdbscan_common_features == set()) or (len(final_hdbscan_common_features) <= 5): # no common features OR not enough for processing\n",
    "    print(' No common features - skipping ahead... ')\n",
    "else:\n",
    "    # Create a DataFrame for the best common features\n",
    "    hdbscan_common_df = test_df.copy()\n",
    "    hdbscan_reduced_common_features_df = hdbscan_common_df.iloc[:,sorted(final_hdbscan_common_features)]\n",
    "    \n",
    "    # Perform clustering on the final set of common features with the common cluster_size\n",
    "    hdbscan_final_common_model = HDBSCAN(min_cluster_size=selected_cluster_size, min_samples=selected_sample_size, metric=most_common_hmetric, cluster_selection_method='eom', store_centers=\"medoid\", allow_single_cluster=np.bool_(True), n_jobs=-1)\n",
    "    hdbscan_final_common_labels = hdbscan_final_common_model.fit_predict(hdbscan_reduced_common_features_df)\n",
    "    hdbscan_final_common_cluster_centers = hdbscan_final_common_model.medoids_\n",
    "    \n",
    "    # Create the dataframes for visualization\n",
    "    hdbscan_final_viz_common_features_df = test_df.copy()\n",
    "    hdbscan_final_reduced_common_features_df = kmedoids_final_viz_common_features_df.iloc[:,sorted(final_hdbscan_common_features)]\n",
    "    hdbscan_final_reduced_common_features_df['HDBSCAN Clusters'] = hdbscan_final_common_labels\n",
    "    \n",
    "    hdbscan_final_COMMON_complete_features_df = test_df.copy()\n",
    "    hdbscan_final_COMMON_complete_features_df['HDBSCAN Clusters'] = hdbscan_final_common_labels\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Create a DataFrame for the best combined features\n",
    "hdbscan_combined_df = test_df.copy()\n",
    "hdbscan_reduced_combined_features_df = hdbscan_combined_df.iloc[:,sorted(final_hdbscan_combined_features)]\n",
    "\n",
    "# Perform clustering on the final set of combined features with the common cluster_size\n",
    "hdbscan_final_combined_model = HDBSCAN(min_cluster_size=selected_cluster_size, min_samples=selected_sample_size, metric=most_common_hmetric, cluster_selection_method='eom', store_centers=\"medoid\", allow_single_cluster=np.bool_(True), n_jobs=-1)\n",
    "hdbscan_final_combined_labels = hdbscan_final_combined_model.fit_predict(hdbscan_reduced_combined_features_df)\n",
    "hdbscan_final_combined_cluster_centers = hdbscan_final_combined_model.medoids_\n",
    "\n",
    "# Create the dataframes for visualization\n",
    "hdbscan_final_viz_combined_features_df = test_df.copy()\n",
    "hdbscan_final_reduced_combined_features_df = hdbscan_final_viz_combined_features_df.iloc[:,sorted(final_hdbscan_combined_features)]\n",
    "hdbscan_final_reduced_combined_features_df['HDBSCAN Clusters'] = hdbscan_final_combined_labels\n",
    "\n",
    "hdbscan_final_COMBINED_complete_features_df = test_df.copy()\n",
    "hdbscan_final_COMBINED_complete_features_df['HDBSCAN Clusters'] = hdbscan_final_combined_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71262018-b628-4e22-a62c-bde1ac74b06c",
   "metadata": {},
   "source": [
    "### Generate reports to explore the clustering results (reduced feature set & complete feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa49a0b-3f95-48c1-bdb8-71595170b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Create YData reports to explore the HDBSCAN feature relationships\n",
    "# DataFrames and configuration for the reports\n",
    "\n",
    "if (final_hdbscan_common_features == set()) or (len(final_hdbscan_common_features) <= 5): # no common features OR not enough for processing\n",
    "    print(' No common features ... ')\n",
    "    now = str(time.time_ns()) # Create a timestamp for unique filename sets\n",
    "    reports_info = [\n",
    "        {\n",
    "            'df': hdbscan_final_reduced_combined_features_df,\n",
    "            'config_file': 'config_ELR.yml',\n",
    "            'output_file': 'HDBSCAN_Final_REDUCED_COMBINED-Features_Report-' + now + '.html'\n",
    "        },\n",
    "        {\n",
    "            'df': hdbscan_final_COMBINED_complete_features_df,\n",
    "            'config_file': 'config_ELR.yml',\n",
    "            'output_file': 'HDBSCAN_Final_COMPLETE_COMBINED-Features_Report-' + now + '.html'\n",
    "        }\n",
    "    ]\n",
    "else:\n",
    "    now = str(time.time_ns()) # Create a timestamp for unique filename sets\n",
    "    reports_info = [\n",
    "        {\n",
    "            'df': hdbscan_final_reduced_common_features_df,\n",
    "            'config_file': 'config_ELR.yml',\n",
    "            'output_file': 'HDBSCAN_Final_REDUCED_COMMON-Features_Report-' + now + '.html'\n",
    "        },\n",
    "        {\n",
    "            'df': hdbscan_final_COMMON_complete_features_df,\n",
    "            'config_file': 'config_ELR.yml',\n",
    "            'output_file': 'HDBSCAN_Final_COMPLETE_COMMON-Features_Report-' + now + '.html'\n",
    "        },\n",
    "        {\n",
    "            'df': hdbscan_final_reduced_combined_features_df,\n",
    "            'config_file': 'config_ELR.yml',\n",
    "            'output_file': 'HDBSCAN_Final_REDUCED_COMBINED-Features_Report-' + now + '.html'\n",
    "        },\n",
    "        {\n",
    "            'df': hdbscan_final_COMBINED_complete_features_df,\n",
    "            'config_file': 'config_ELR.yml',\n",
    "            'output_file': 'HDBSCAN_Final_COMPLETE_COMBINED-Features_Report-' + now + '.html'\n",
    "        }\n",
    "    ]\n",
    "# Use joblib to run the report generations in parallel\n",
    "# n_jobs=-1 uses all available CPUs\n",
    "Parallel(n_jobs=4)(delayed(generate_report)(\n",
    "    info['df'], info['config_file'], info['output_file']) for info in reports_info)\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f\" ^^^ Final HDBSCAN Clustering Report building in {stop - start:0.4f} seconds ^^^ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c0b50-fdcf-42c1-880b-082c430ec556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566a4be-58f5-4e7c-b5ce-30aa131c25dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83af9d0-ee77-4422-a6e9-c31d957ec1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092fe3ee-1da6-4aee-bb60-28af77382770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25326096-9d89-4642-9abb-1503f0d127f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de655e-23f1-43f3-9ec2-45f1cec2e707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb80d45-ecb6-45d6-96a9-738786e8abd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b32878a-bb2c-4a67-931c-571830cf6ce8",
   "metadata": {},
   "source": [
    "### Write Results to Project Database ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c3716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the config from the .env file\n",
    "load_dotenv()\n",
    "MONGODB_URI = os.environ['MONGODB_URI']\n",
    "\n",
    "# Connect to the database engine\n",
    "client = MongoClient(MONGODB_URI)\n",
    "\n",
    "# connect to the project db\n",
    "db = client['ExpectLifeRedux']\n",
    "\n",
    "# get a reference to the data collection\n",
    "#gov_data = db['Encoded_Gov_Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da1475-8efb-4aa2-b466-85c3376357af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefered method - use PyMongoArrow - write the dataframes to the database\n",
    "write(db.Cluster_Unscaled_Complete, viz_df)\n",
    "write(db.Cluster_Scaled_Complete, complete_df)\n",
    "write(db.Cluster_PCA_Complete, complete_pca_df)\n",
    "write(db.Cluster_KMedoids_Reduced_Features, kmedoids_final_reduced_features_df)\n",
    "write(db.Cluster_KMedoids_Complete_Features, kmedoids_final_complete_features_df)\n",
    "write(db.Cluster_HDBSCAN_Reduced_Features, hdbscan_final_reduced_features_df)\n",
    "write(db.Cluster_HDBSCAN_Complete_Features, hdbscan_final_complete_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7d096-879d-41f1-8766-20b3d3f0332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmedoids_cluster_centers_df = pd.DataFrame(kmedoids_final_cluster_centers)\n",
    "#write(db.Cluster_KMedoids_Centers, kmedoids_cluster_centers_df)\n",
    "\n",
    "# Create the dataframe\n",
    "#kmedoids_labels_df = pd.DataFrame(kmedoids_final_labels)\n",
    "#write(db.Cluster_KMedoids_Labels, kmedoids_labels_df)\n",
    "\n",
    "# Create the dataframe\n",
    "#hdbscan_centers_df = pd.DataFrame(hdbscan_final_cluster_centers)\n",
    "#write(db.Cluster_HDBSCAN_Centers, hdbscan_centers_df)\n",
    "\n",
    "# Create the dataframe\n",
    "#hdbscan_labels_df = pd.DataFrame(hdbscan_final_labels)\n",
    "#write(db.Cluster_HDBSCAN_Labels, hdbscan_labels_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8c48d-5316-4a72-b570-52a0e8d5f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmedoids_best_features_df = pd.DataFrame()\n",
    "kmedoids_best_features_df['Features'] = best_kmedoid_features\n",
    "kmedoids_best_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502e8e8f-1ddb-4ee5-883d-45049831782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hbdbscan_best_features_df = pd.DataFrame()\n",
    "hbdbscan_best_features_df['Features'] = best_hdbscan_features\n",
    "hbdbscan_best_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603704b6-bded-4600-987d-549c94f9898a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
