{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1f77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "import pymongoarrow as pma\n",
    "from pymongoarrow.api import write\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import numba\n",
    "import random\n",
    "from numba.typed import List\n",
    "from functools import reduce\n",
    "from itertools import combinations\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import hvplot.pandas\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Suppress YData profile report generation warnings - no actual problems to resolve.\n",
    "from warnings import simplefilter \n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4914ab1-f2b0-413e-b103-fe2df9834868",
   "metadata": {},
   "source": [
    "### Load data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bdb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the config from the .env file\n",
    "load_dotenv()\n",
    "MONGODB_URI = os.environ['MONGODB_URI']\n",
    "\n",
    "# Connect to the database engine\n",
    "client = MongoClient(MONGODB_URI)\n",
    "\n",
    "# connect to the project db\n",
    "db = client['ExpectLifeRedux']\n",
    "\n",
    "# get references to the data collections\n",
    "data1 = db['ELR_Input_Data']\n",
    "data2 = db['Encoded_Gov_Data']\n",
    "data3 = db['Encoded_SSS_Data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018184e0-2afd-4a76-8fae-42b95b75a610",
   "metadata": {},
   "source": [
    "### Create DataFrames, Adjust columns, and set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the ELR_Data collection\n",
    "combined_df = pd.DataFrame(list(data1.find()))\n",
    "\n",
    "# Create a dataframe from the Gov_Clusters collection\n",
    "gc_df = pd.DataFrame(list(data2.find()))\n",
    "\n",
    "# Create a dataframe from the SSS_Cluster collection\n",
    "sc_df = pd.DataFrame(list(data3.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abef86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a copy of the original combined dataframe\n",
    "ori_df = combined_df.copy()\n",
    "\n",
    "# Drop the database id data and refresh the index\n",
    "combined_df = combined_df.drop(['_id', 'Country', 'Year'], axis=1)\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "combined_df = combined_df.set_index('Country_Year')\n",
    "combined_df = combined_df.drop(['Gov Type', 'SSS Type'], axis=1)\n",
    "# Sort by index\n",
    "combined_df = combined_df.sort_index()\n",
    "\n",
    "gc_df = gc_df.drop(['_id'], axis=1)\n",
    "gc_df = gc_df.reset_index(drop=True)\n",
    "gc_df = gc_df.set_index('Country_Year')\n",
    "# Sort by index\n",
    "gc_df = gc_df.sort_index()\n",
    "\n",
    "sc_df = sc_df.drop(['_id'], axis=1)\n",
    "sc_df = sc_df.reset_index(drop=True)\n",
    "# Sort by index\n",
    "sc_df = sc_df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95669875-e0d5-4e4a-bc5b-5fbfc39f886a",
   "metadata": {},
   "source": [
    "### Scale the numeric data before combining with binary encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ab0b4-29e8-4cbd-88ec-2d052f24095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the column labels so they can be reapplied after data scaling\n",
    "numeric_col_names = combined_df.columns.tolist()\n",
    "\n",
    "# Standardize the data with MaxAbsScaler().\n",
    "scaler = MaxAbsScaler()\n",
    "scaled_nda = scaler.fit_transform(combined_df)\n",
    "\n",
    "# Convert the scaled-encoded data back to a DataFrame (nda = Numpy Data Array)\n",
    "scaled_df = pd.DataFrame(scaled_nda, index=combined_df.index)\n",
    "\n",
    "# Apply the column labels to ensure the data is properly identified\n",
    "scaled_df = scaled_df.set_axis(numeric_col_names, axis=1)\n",
    "scaled_df = scaled_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802bb18-b76f-4e00-b697-30848df089a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d95d00-fc5a-4abd-b3bf-22a68ffd8b12",
   "metadata": {},
   "source": [
    "### Assemble the complete dataset by merging the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038f1bb-5a6e-4986-ad1e-0a125e1977b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the cluster DataFrames with the primary data.\n",
    "frames = [scaled_df, gc_df, sc_df]\n",
    "merge_frames_df = reduce(lambda left,right: pd.merge(left,right,how='left',on='Country_Year'),frames)\n",
    "\n",
    "complete_df = merge_frames_df.copy().reset_index(drop=True)\n",
    "complete_df = complete_df.set_index('Country_Year')\n",
    "complete_df = complete_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee9eb4d-dd66-4301-a257-eedcce8f9886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0ab47-daca-4712-ad18-4f1941612f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the column labels so they can be reapplied after PFA\n",
    "complete_col_names = complete_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86200281-b82e-4011-b406-b7af7490a072",
   "metadata": {},
   "source": [
    "### Assemble the complete visualization dataset by merging the unscaled numeric and binary encoded frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39bb8b6-e3d6-4280-8f68-68e618f98223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the visualization dataframe\n",
    "frames = [combined_df, gc_df, sc_df]\n",
    "merge_df = reduce(lambda left,right: pd.merge(left,right,how='left',on='Country_Year'),frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a336bc-faeb-4253-afc1-60ce43dc27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df = merge_df.copy().reset_index(drop=True)\n",
    "viz_df = viz_df.set_index('Country_Year')\n",
    "viz_df = viz_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4d02b-50d4-4623-ba4c-4eaeee1644ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b48e54-5797-4705-a916-794f89244b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that generates a profile report and saves it to a file\n",
    "def generate_report(df, config_file, output_file):\n",
    "    profile = ProfileReport(df, config_file=config_file)\n",
    "    profile.to_file(output_file)\n",
    "    print(f\"Report {output_file} generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a00e4-6bd4-4b45-a029-3c6b2ccbdf55",
   "metadata": {},
   "source": [
    "### Determine number of components for PCA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478eccf5-7a78-4ebc-97fe-bca00fcca885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the viable PCA components for a given dataset\n",
    "def compute_pca(input_data):  \n",
    "    # Fit PCA on actual data\n",
    "    pca_actual = PCA(svd_solver='full').fit(input_data)\n",
    "    \n",
    "    cumulative_variance_ratio = np.cumsum(pca_actual.explained_variance_ratio_)\n",
    "    plt.plot(cumulative_variance_ratio)\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Cumulative explained variance')\n",
    "    plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89af626-cc4a-455f-b6a4-675528d1c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of appropriate components for PCA (scaled (combined) data)\n",
    "compute_pca(complete_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f912c2-30eb-490f-887b-4507fa150253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform PCA for the provided data\n",
    "def perform_pca(input_data, name, n_comp):\n",
    "    pca = PCA(n_components=n_comp, random_state=42)\n",
    "    pca_out = pca.fit_transform(input_data)\n",
    "\n",
    "    # Create a DataFrame with the principal components.\n",
    "    columnz =[]\n",
    "\n",
    "    for i in range(1,n_comp+1):\n",
    "        columnz.append(name + '_pc'+str(i))\n",
    "    \n",
    "    out_df = pd.DataFrame(data=pca_out, columns=columnz)\n",
    "\n",
    "    out_df['Country_Year'] = ori_df['Country_Year']\n",
    "    out_df = out_df.reset_index(drop=True)\n",
    "    out_df = out_df.set_index('Country_Year')\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c795a-b94a-4474-829b-abe5edc1a0e3",
   "metadata": {},
   "source": [
    "#### Use the PCA data to guide component number selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0732e98-086e-4a22-9a0e-6f3a21cdf96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the complete_df.\n",
    "complete_pca_df = perform_pca(complete_df, 'Complete', 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a4cd02-dafd-40b0-a594-89b72d5412e6",
   "metadata": {},
   "source": [
    "### Determine the number of (KMedoids) clusters for this complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9543e4-62e6-4546-b84b-f066943f8ec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use KMedoids and compute Davies-Bouldin scores, elbow curve, and silhouette scores to determine the optimal number of clusters.\n",
    "def compute_kmedoids_cluster_metrics(data_in, n_clusters):\n",
    "    \"\"\"\n",
    "    Function to compute cluster metrics for a given number of clusters.\n",
    "    This function will be called in parallel.\n",
    "    \"\"\"\n",
    "    # Initialize the clusterer with n_clusters value and random state for reproducibility\n",
    "    clusterer = KMedoids(n_clusters=n_clusters, init='k-medoids++', random_state=42)\n",
    "    cluster_labels = clusterer.fit_predict(data_in)\n",
    "    \n",
    "    # Compute the scores for various metrics\n",
    "    davies_bouldin = davies_bouldin_score(data_in, cluster_labels)\n",
    "    inertia = clusterer.inertia_\n",
    "    silhouette_avg = silhouette_score(data_in, cluster_labels)\n",
    "    \n",
    "    return n_clusters, davies_bouldin, inertia, silhouette_avg\n",
    "    \n",
    "def compute_clusters_parallel(data_in, max_clusters, n_jobs=-1):\n",
    "    # Parallel computation of the cluster metrics for each number of clusters from 2 to max_clusters\n",
    "    parallel = Parallel(n_jobs=n_jobs)\n",
    "    kmedoids_cluster_metrics_list = parallel(delayed(compute_kmedoids_cluster_metrics)(data_in, n_clusters)\n",
    "                                    for n_clusters in range(2, max_clusters + 1))\n",
    "\n",
    "    # Creating a DataFrame to store the clustering metrics\n",
    "    kmedoids_cluster_metrics_df = pd.DataFrame(kmedoids_cluster_metrics_list,\n",
    "                                      columns=['Num_Clusters', 'Davies_Bouldin', 'Inertia', 'Silhouette_Avg'])\n",
    "\n",
    "    return kmedoids_cluster_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c1b70-8412-41d2-839b-994f07ecf620",
   "metadata": {},
   "source": [
    "### Compute KMedoids clusters and metrics for the complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0370c3-fb9a-4356-92c9-1cf6d9bcea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Create a DataFrame of the results for further analysis downstream - Compute clusters for the complete dataframe\n",
    "non_pca_kmedoid_cluster_scores_df = compute_clusters_parallel(data_in=complete_df, max_clusters=60, n_jobs=-1)\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f\"KMedoids Clustering Execution in {stop - start:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cdf057-cb08-43df-b27a-8ff10f8f75e8",
   "metadata": {},
   "source": [
    "### Compute KMedoids clusters and metrics for the PCA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d93c6-cb5f-4393-ac1f-d4b2af717816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "pca_kmedoid_cluster_start = time.perf_counter()\n",
    "\n",
    "# Create a DataFrame of the results for further analysis downstream - Compute clusters for the complete dataframe with PCA\n",
    "pca_kmedoid_cluster_scores_df= compute_clusters_parallel(data_in=complete_pca_df, max_clusters=60, n_jobs=-1)\n",
    "\n",
    "# Stop timing\n",
    "pca_kmedoid_cluster_stop = time.perf_counter()\n",
    "\n",
    "print(f\"PCA KMedoids Clustering Execution in {pca_kmedoid_cluster_stop - pca_kmedoid_cluster_start:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7becd21-1243-4ffe-b608-0044573a79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower scores are better\n",
    "pca_kmedoid_cluster_scores_df.hvplot.scatter(x='Num_Clusters', y='Davies_Bouldin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de83577-fec3-4dd5-9a55-b2ecf2a7da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher scores are better\n",
    "pca_kmedoid_cluster_scores_df.hvplot.scatter(x='Num_Clusters', y='Silhouette_Avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ef6c6-5e40-483d-81ad-f8d7dee6e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "cluster_model = KMedoids(n_clusters=55, init='k-medoids++', random_state=42)\n",
    "\n",
    "# Fit the model and predict labels\n",
    "cluster_model.fit_predict(complete_pca_df)\n",
    "\n",
    "# Add the predicted class columns to the visualization dataset\n",
    "complete_pca_viz_df = viz_df.copy()\n",
    "complete_pca_viz_df['KMedoids Clusters'] = cluster_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4a972-e7fe-4ca3-b1d3-d866f4715594",
   "metadata": {},
   "source": [
    "## Principal Feature Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2bf07-10e8-4c74-a7db-98a407b85926",
   "metadata": {},
   "source": [
    "#### Define functions to select dataset features that provide relevant information for clustering. \n",
    "##### Only important features are used to compute clusters from the complete (non-pca) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b942d4e6-7ad2-4b99-ac0f-8b8fca9d1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Custom processing function to override limitations of Numba compatibility with Numpy features\n",
    "@numba.jit(nopython=True)\n",
    "def custom_mean(arr, axis=0):\n",
    "    if arr.ndim == 1:\n",
    "        return arr.sum() / arr.shape[0]\n",
    "    elif arr.ndim == 2:\n",
    "        if axis == 0:\n",
    "            return arr.sum(axis=0) / arr.shape[0]\n",
    "        elif axis == 1:\n",
    "            return arr.sum(axis=1) / arr.shape[1]\n",
    "    raise ValueError(\"custom_mean function received an array that it can't handle with axis = {}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54052f-8069-4a6f-9ba9-b32b80222f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Function: Calinski Harbasz Score Calculation\n",
    "def calculate_calinski_harbasz(np_array, labels):\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        calinski_harbasz = calinski_harabasz_score(np_array, labels)\n",
    "        return calinski_harbasz\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34286573-3ba9-42cc-86d0-9e23e73f708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Function: Davies-Bouldin Score Calculation\n",
    "def calculate_davies_bouldin(np_array, labels):\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        davies_bouldin = davies_bouldin_score(np_array, labels)\n",
    "        return davies_bouldin\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131fdb1-e0c3-4534-abd8-641895262dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Function: Silhouette Coefficient Calculation\n",
    "def calculate_silhouette(np_array, labels):\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        silhouette_val = silhouette_score(np_array, labels)\n",
    "        return silhouette_val\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fad2f8-0256-472a-85e1-5ae23b2a65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Function: Scatter Separability Calculation\n",
    "def calculate_scatter_separability(np_array, labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_features = np_array.shape[1]\n",
    "    overall_mean = custom_mean(np_array, axis=0)\n",
    "    \n",
    "    S_w = np.zeros((n_features, n_features))\n",
    "    S_b = np.zeros((n_features, n_features))\n",
    "\n",
    "    for label in unique_labels:\n",
    "        X_k = np_array[labels == label]\n",
    "        mean_k = custom_mean(X_k, axis=0).reshape(n_features, 1)\n",
    "        diff = X_k - mean_k.T\n",
    "        S_w += np.dot(diff.T, diff)\n",
    "        mean_diff = mean_k - overall_mean.reshape(n_features, 1)\n",
    "        S_b += X_k.shape[0] * np.dot(mean_diff, mean_diff.T)\n",
    "\n",
    "    # Check if S_w is invertible\n",
    "    if np.linalg.cond(S_w) < 1/sys.float_info.epsilon:\n",
    "        final_ssc = np.trace(np.linalg.inv(S_w).dot(S_b))\n",
    "    else:\n",
    "        final_ssc = 0\n",
    "\n",
    "    return final_ssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52cc396-7659-4138-a334-246467295a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Function: Normalization of criterion values to remove bias due to number of clusters - Numba acceleration\n",
    "@numba.jit(nopython=True)\n",
    "def cross_projection_normalization(clustering_medoids, scatter_criteria_score, silhouette_criteria_score, davies_bouldin_score, calinski_harbasz_index):\n",
    "    n_clusters = len(clustering_medoids)\n",
    "    projections = np.zeros((n_clusters, n_clusters))\n",
    "\n",
    "    for j in range(n_clusters):\n",
    "        for k in range(j + 1, n_clusters):\n",
    "            medoid_j = clustering_medoids[j]\n",
    "            medoid_k = clustering_medoids[k]\n",
    "            distance = np.linalg.norm(medoid_j - medoid_k)\n",
    "            projections[j][k] = distance\n",
    "            projections[k][j] = distance\n",
    "\n",
    "    # Flatten the array and filter non-zero distances then calculate the mean\n",
    "    flat_projections = projections.ravel()\n",
    "    non_zero_projections = flat_projections[flat_projections > 0]\n",
    "    mean_projection = np.mean(non_zero_projections)\n",
    "\n",
    "    # Normalizing the criteria scores with the mean of projections\n",
    "    # Adjusting the formula to consider Davies-Bouldin Score. Recall: For Davies-Bouldin, lower is better.\n",
    "    # We add 1 to the normalization_factor to ensure it doesn't lead to division by zero or negative values.\n",
    "\n",
    "    # Combined normalization factor incorporates all metrics.\n",
    "    normalization_factor = (1 + mean_projection + davies_bouldin_score) \n",
    "\n",
    "    normalized_score = (scatter_criteria_score + silhouette_criteria_score + calinski_harbasz_index) / normalization_factor\n",
    "\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5e799-345e-477d-a30a-d6b7c4a936b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper function for Sequential Forward Search\n",
    "def evaluate_feature_subset(subset_array, np_array, cluster_labels, clustering_medoids):\n",
    "    scatter_separability = calculate_scatter_separability(subset_array, cluster_labels)\n",
    "    silhouette_score = calculate_silhouette(subset_array, cluster_labels)\n",
    "    davies_bouldin_score = calculate_davies_bouldin(subset_array, cluster_labels)\n",
    "    calinski_harbasz_index = calculate_calinski_harbasz(subset_array, cluster_labels)\n",
    "    normalized_score = cross_projection_normalization(clustering_medoids, scatter_separability, silhouette_score, davies_bouldin_score, calinski_harbasz_index)\n",
    "\n",
    "    return normalized_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9072a4f6-c26b-41aa-a69a-7d9ad1593c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Main Function - Function that orchestrates sequential forward search for important features and \n",
    "# evaluates different numbers of clusters to locate the optimal value\n",
    "def optimal_feature_clusters(np_array, clustering_algorithm):\n",
    "    np_array_feature_indices = np_array.shape[1]\n",
    "    available_indices = set(range(np_array_feature_indices))  # Initial set of available indices\n",
    "    interim_features = set()\n",
    "    starter_set = set()\n",
    "    n_features = len(available_indices)\n",
    "    initial_k = np.array([3, 4, 6, 7, 8, 9, 10, 11, 12])\n",
    "    \n",
    "    random.seed(42)\n",
    "    evaluate = True\n",
    "    init_k = 2\n",
    "    best_k = init_k\n",
    "    best_score = 0\n",
    "    best_combination_score = 0\n",
    "    processed_features = 0\n",
    "    combination_array = []\n",
    "    starter_set_size = max(1, int(0.1 * n_features))\n",
    "\n",
    "    while evaluate:     # Simple test to enable and continue evaluation.\n",
    "        print(f' Start processing with k = {init_k} and {len(available_indices)} available_indices... ')\n",
    "\n",
    "        if clustering_algorithm == 'kmedoids':\n",
    "            clustering_instance = KMedoids(n_clusters=init_k, init='k-medoids++', metric='manhattan', random_state=42)          \n",
    "        elif clustering_algorithm == 'hdbscan':\n",
    "            clustering_instance = HDBSCAN(min_cluster_size=10, min_samples=20, cluster_selection_method='eom', store_centers=\"medoid\", allow_single_cluster=np.bool_(True), n_jobs=-1)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported clustering algorithm\")\n",
    "\n",
    "        while processed_features < 0.8 * n_features:\n",
    "            if len(interim_features) == 0 or interim_features is None:\n",
    "                print(' interim_features is currently empty - creating starter_set from scratch ')\n",
    "                starter_set = np.random.choice(n_features, starter_set_size)\n",
    "            else:\n",
    "                print(' Interim_features exist - determining features available for use in starter_set ... ')\n",
    "                remaining_indices = available_indices - interim_features\n",
    "                \n",
    "                if len(remaining_indices) < starter_set_size:\n",
    "                    print(' Almost at the end - using remaining features as the starter set... ')\n",
    "                    starter_set = np.random.choice(list(remaining_indices), len(remaining_indices), replace=False)\n",
    "                else:\n",
    "                    print(' Creating a starter set of fresh features ... ')\n",
    "                    starter_set = np.random.choice(list(remaining_indices), starter_set_size, replace=False)\n",
    "                    \n",
    "            best_feature = None\n",
    "            best_score = 10\n",
    "    \n",
    "            for feature in range(n_features):\n",
    "                if feature not in starter_set and feature not in interim_features:  # Check for both conditions\n",
    "                    combined_features = np.concatenate([starter_set, [feature]])\n",
    "                    subset_array = np.hstack([np_array[:, combined_features]])\n",
    "                    current_labels = clustering_instance.fit_predict(subset_array)\n",
    "                    if clustering_algorithm == 'kmedoids':\n",
    "                        clustering_medoids = clustering_instance.cluster_centers_\n",
    "                    elif clustering_algorithm == 'hdbscan':\n",
    "                        clustering_medoids = clustering_instance.medoids_\n",
    "                    # Score the feature                  \n",
    "                    normalized_score = evaluate_feature_subset(subset_array, np_array, current_labels, clustering_medoids)\n",
    "    \n",
    "                    # Update best feature if necessary\n",
    "                    if normalized_score > best_score:\n",
    "                        best_score = normalized_score\n",
    "                        best_feature = feature\n",
    "        \n",
    "                # Ensure best_feature is not already in interim_features before appending\n",
    "                if best_feature is not None and best_feature not in interim_features:\n",
    "                    interim_features.add(best_feature)\n",
    "                    print(f' Found a new interim feature - {len(interim_features)} found so far... ')\n",
    "                    available_indices.remove(best_feature)\n",
    "                    \n",
    "            processed_features += len(starter_set)  # Account for multiple features in starter set\n",
    "            print(f' Processed {processed_features} % of Features')\n",
    "        best_combination_score = best_score\n",
    "        subset_array = []\n",
    "        starter_set = set()\n",
    "        print(f'^^^^^^^^^^^^^^^^^^^^^^^^^^{len(interim_features)} Initial features identified - continuing with combination evaluation ^^^^^^^^^^^^^^^^^')\n",
    "        \n",
    "        for init_k in initial_k:\n",
    "            if clustering_algorithm == 'kmedoids':\n",
    "                print(f'****** k-value set to {init_k} ***************************')\n",
    "                clustering_instance = KMedoids(n_clusters=init_k, init='k-medoids++', metric='manhattan', random_state=42)  \n",
    "            # Process combinations with the current selected features\n",
    "\n",
    "            best_add = None\n",
    "            \n",
    "            for i in available_indices:\n",
    "                # Create combination subset\n",
    "                #print(f' Evaluate combined features - current feature = {i} ')\n",
    "                combination_array = np.hstack([np_array[:, [i]], np_array[:, interim_features]])\n",
    "                current_labels = clustering_instance.fit_predict(combination_array)\n",
    "                if clustering_algorithm == 'kmedoids':\n",
    "                    clustering_medoids = clustering_instance.cluster_centers_\n",
    "                elif clustering_algorithm == 'hdbscan':\n",
    "                    clustering_medoids = clustering_instance.medoids_\n",
    "    \n",
    "                # Score the combination            \n",
    "                normalized_score = evaluate_feature_subset(combination_array, np_array, current_labels, clustering_medoids)\n",
    "                    \n",
    "                # Update best combination if necessary\n",
    "                if normalized_score > best_combination_score:\n",
    "                    best_combination_score = normalized_score\n",
    "                    print(f'  ++++++++ Best Combination Score Updated = {best_combination_score} ++++++++')\n",
    "                    best_add = i\n",
    "                    best_k = init_k\n",
    "    \n",
    "            # If a better combination was found, add its feature to selected features\n",
    "            if best_add is not None and best_add not in interim_features:\n",
    "                interim_features.add(best_add)\n",
    "                available_indices.remove(best_add)\n",
    "                print(f' number of interim features = {len(interim_features)} and number of available_indices = {len(available_indices)}... ')\n",
    "            else:\n",
    "                #available_indices.remove(best_add)\n",
    "                print('xxxxx  No further progress >>>> Moving to next K >>>>>>>')\n",
    "\n",
    "        evaluate = False\n",
    "        print(f' Processing completed - Total number of identified features = {len(interim_features)}')\n",
    "\n",
    "    return best_k, interim_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b4c3a-fd1c-4330-8fc6-8bcce04431f8",
   "metadata": {},
   "source": [
    "### Perform PFA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34f8db-361e-41de-a411-4208e9890670",
   "metadata": {},
   "source": [
    "#### KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c4fd7-6304-4b22-bfa7-0ffb3657bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "best_k = -99\n",
    "best_kmedoid_features = []\n",
    "\n",
    "complete1_df = complete_df.copy()\n",
    "complete1_np = complete1_df.to_numpy()\n",
    "# Run the experiment using the complete (non-pca) dataframe and identify the clustering algorithm by name.\n",
    "best_k, best_kmedoid_features = optimal_feature_clusters(complete1_np, 'kmedoids')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^ RUN #1 --- PFA KMedoids Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' Best k = {best_k}')\n",
    "print(f' best features = {best_kmedoid_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3fb7e3-4a66-4647-acd5-f90e3f711a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "best_k2 = -99\n",
    "best_kmedoid_features2 = []\n",
    "\n",
    "complete2_df = complete_df.copy()\n",
    "complete2_np = complete2_df.to_numpy()\n",
    "# Run the experiment using the complete (non-pca) dataframe and identify the clustering algorithm by name.\n",
    "best_k2, best_kmedoid_features2 = optimal_feature_clusters(complete2_np, 'kmedoids')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^ RUN #2 --- PFA KMedoids Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' Best k = {best_k2}')\n",
    "print(f' best features = {best_kmedoid_features2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07fa40-b99c-4065-8890-922cbc609839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "best_k3 = -99\n",
    "best_kmedoid_features3 = []\n",
    "\n",
    "complete3_df = complete_df.copy()\n",
    "complete3_np = complete3_df.to_numpy()\n",
    "# Run the experiment using the complete (non-pca) dataframe and identify the clustering algorithm by name.\n",
    "best_k3, best_kmedoid_features3 = optimal_feature_clusters(complete3_np, 'kmedoids')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^ RUN #3 --- PFA KMedoids Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' Best k = {best_k3}')\n",
    "print(f' best features = {best_kmedoid_features3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae22b35-8d36-4c24-a96f-a1aab320dfae",
   "metadata": {},
   "source": [
    "### Perform clustering with the reduced feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d1d8b-dacc-43f1-bfc8-3c506dfa0ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the selected features for the final KMedoids clustering\n",
    "kmedoids_final_df = complete_df.copy()\n",
    "kmedoids_reduced_features_df = kmedoids_final_df.iloc[:,best_kmedoid_features]\n",
    "#kmedoids_reduced_features_df = complete_df[best_kmedoid_features].copy()\n",
    "\n",
    "# Perform clustering on the final set of features\n",
    "kmedoids_final_model = KMedoids(n_clusters=best_k, init='k-medoids++', metric='manhattan', random_state=42)\n",
    "kmedoids_final_labels = kmedoids_final_model.fit_predict(kmedoids_reduced_features_df)\n",
    "kmedoids_final_cluster_centers = kmedoids_final_model.cluster_centers_\n",
    "\n",
    "# Create the dataframes for visualization\n",
    "kmedoids_final_reduced_features_df = viz_df[best_kmedoid_features].copy()\n",
    "kmedoids_final_reduced_features_df['KMedoids Clusters'] = kmedoids_final_labels\n",
    "\n",
    "kmedoids_final_complete_features_df = viz_df.copy()\n",
    "kmedoids_final_complete_features_df['KMedoids Clusters'] = kmedoids_final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8962f17-85de-427a-9c83-9a1ff4b6f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Create YData reports to explore the KMedoids feature relationships\n",
    "# DataFrames and configuration for the reports\n",
    "reports_info = [\n",
    "    {\n",
    "        'df': kmedoids_final_reduced_features_df,\n",
    "        'config_file': 'config_ELR.yml',\n",
    "        'output_file': 'KMedoids_Final_Reduced-Features_Report.html'\n",
    "    },\n",
    "    {\n",
    "        'df': kmedoids_final_complete_features_df,\n",
    "        'config_file': 'config_ELR.yml',\n",
    "        'output_file': 'KMedoids_Final_Complete-Features_Report.html'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Use joblib to run the report generations in parallel\n",
    "# n_jobs=-1 uses all available CPUs\n",
    "Parallel(n_jobs=-1)(delayed(generate_report)(\n",
    "    info['df'], info['config_file'], info['output_file']) for info in reports_info)\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^ Final KMedoids Clustering Report building in {stop - start:0.4f} seconds ^^^ ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c11ed-9958-4afe-92ec-19c4ee96efe3",
   "metadata": {},
   "source": [
    "#### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14c35a-530d-4b64-b522-d62b091f3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "complete4_df = complete_df.copy()\n",
    "complete4_np = complete4_df.to_numpy()\n",
    "\n",
    "best_hdbscan_features = []\n",
    "# Run the experiment using the complete (non-pca) dataframe\n",
    "not_used, best_hdbscan_features = optimal_feature_clusters(complete4_np, 'hdbscan')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^RUN #1 --- PFA HDBSCAN Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' best features 1 = {best_hdbscan_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0cbc11-897d-4960-a7eb-7da8db806d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "complete5_df = complete_df.copy()\n",
    "complete5_np = complete5_df.to_numpy()\n",
    "\n",
    "best_hdbscan_features_2 = []\n",
    "# Run the experiment using the complete (non-pca) dataframe\n",
    "not_used, best_hdbscan_features_2 = optimal_feature_clusters(complete5_np, 'hdbscan')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^RUN #2 --- PFA HDBSCAN Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' best features 2 = {best_hdbscan_features_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea4ff63-db44-4438-a75f-40cb769f5de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "complete6_df = complete_df.copy()\n",
    "complete6_np = complete6_df.to_numpy()\n",
    "\n",
    "best_hdbscan_features_3 = []\n",
    "# Run the experiment using the complete (non-pca) dataframe\n",
    "not_used, best_hdbscan_features_3 = optimal_feature_clusters(complete6_np, 'hdbscan')\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f' ^^^RUN #3 --- PFA HDBSCAN Clustering Execution in {stop - start:0.4f} seconds ^^^ ')\n",
    "print(f' best features 3  = {best_hdbscan_features_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aca15f-803b-421e-adbf-34a91db94544",
   "metadata": {},
   "source": [
    "### Perform clustering with the reduced feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6ca56-6246-4929-af4d-3c8e3b2c4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the selected features for the final HDBSCAN clustering\n",
    "hdbscan_reduced_features_np = np.hstack([complete_np[:, best_hdbscan_features]])\n",
    "hdbscan_reduced_features_df = pd.DataFrame(hdbscan_reduced_features_np)\n",
    "#hdbscan_reduced_features_df = complete_df[best_hdbscan_features].copy()\n",
    "\n",
    "# Perform clustering on the final set of features\n",
    "hdbscan_final_model = HDBSCAN(min_cluster_size=25, store_centers='medoid', n_jobs=-1)\n",
    "hdbscan_final_labels = hdbscan_final_model.fit_predict(hdbscan_reduced_features_df)\n",
    "hdbscan_final_cluster_centers = hdbscan_final_model.medoids_\n",
    "\n",
    "# Create the dataframes for visualization\n",
    "hdbscan_final_reduced_features_df = viz_df[best_hdbscan_features].copy()\n",
    "hdbscan_final_reduced_features_df['HDBSCAN Clusters'] = hdbscan_final_labels\n",
    "\n",
    "hdbscan_final_complete_features_df = viz_df.copy()\n",
    "hdbscan_final_complete_features_df['HDBSCAN Clusters'] = hdbscan_final_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71262018-b628-4e22-a62c-bde1ac74b06c",
   "metadata": {},
   "source": [
    "### Generate reports to explore the clustering results (reduced feature set & complete feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa49a0b-3f95-48c1-bdb8-71595170b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Create YData reports to explore the HDBSCAN feature relationships\n",
    "# DataFrames and configuration for the reports\n",
    "reports_info = [\n",
    "    {\n",
    "        'df': hdbscan_final_reduced_features_df,\n",
    "        'config_file': 'config_ELR.yml',\n",
    "        'output_file': 'HDBSCAN_Final_Reduced-Features_Report.html'\n",
    "    },\n",
    "    {\n",
    "       'df': hdbscan_final_complete_features_df,\n",
    "       'config_file': 'config_ELR.yml',\n",
    "       'output_file': 'HDBSCAN_Final_Complete-Features_Report.html'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Use joblib to run the report generations in parallel\n",
    "# n_jobs=-1 uses all available CPUs\n",
    "Parallel(n_jobs=-1)(delayed(generate_report)(\n",
    "    info['df'], info['config_file'], info['output_file']) for info in reports_info)\n",
    "\n",
    "# Stop timing\n",
    "stop = time.perf_counter()\n",
    "\n",
    "print(f\" ^^^ Final HDBSCAN Clustering Report building in {stop - start:0.4f} seconds ^^^ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b32878a-bb2c-4a67-931c-571830cf6ce8",
   "metadata": {},
   "source": [
    "### Write Results to Project Database ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c3716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the config from the .env file\n",
    "load_dotenv()\n",
    "MONGODB_URI = os.environ['MONGODB_URI']\n",
    "\n",
    "# Connect to the database engine\n",
    "client = MongoClient(MONGODB_URI)\n",
    "\n",
    "# connect to the project db\n",
    "db = client['ExpectLifeRedux']\n",
    "\n",
    "# get a reference to the data collection\n",
    "#gov_data = db['Encoded_Gov_Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da1475-8efb-4aa2-b466-85c3376357af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefered method - use PyMongoArrow - write the dataframes to the database\n",
    "write(db.Cluster_Unscaled_Complete, viz_df)\n",
    "write(db.Cluster_Scaled_Complete, complete_df)\n",
    "write(db.Cluster_PCA_Complete, complete_pca_df)\n",
    "write(db.Cluster_KMedoids_Reduced_Features, kmedoids_final_reduced_features_df)\n",
    "write(db.Cluster_KMedoids_Complete_Features, kmedoids_final_complete_features_df)\n",
    "write(db.Cluster_HDBSCAN_Reduced_Features, hdbscan_final_reduced_features_df)\n",
    "write(db.Cluster_HDBSCAN_Complete_Features, hdbscan_final_complete_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7d096-879d-41f1-8766-20b3d3f0332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmedoids_cluster_centers_df = pd.DataFrame(kmedoids_final_cluster_centers)\n",
    "#write(db.Cluster_KMedoids_Centers, kmedoids_cluster_centers_df)\n",
    "\n",
    "# Create the dataframe\n",
    "#kmedoids_labels_df = pd.DataFrame(kmedoids_final_labels)\n",
    "#write(db.Cluster_KMedoids_Labels, kmedoids_labels_df)\n",
    "\n",
    "# Create the dataframe\n",
    "#hdbscan_centers_df = pd.DataFrame(hdbscan_final_cluster_centers)\n",
    "#write(db.Cluster_HDBSCAN_Centers, hdbscan_centers_df)\n",
    "\n",
    "# Create the dataframe\n",
    "#hdbscan_labels_df = pd.DataFrame(hdbscan_final_labels)\n",
    "#write(db.Cluster_HDBSCAN_Labels, hdbscan_labels_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8c48d-5316-4a72-b570-52a0e8d5f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmedoids_best_features_df = pd.DataFrame()\n",
    "kmedoids_best_features_df['Features'] = best_kmedoid_features\n",
    "kmedoids_best_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502e8e8f-1ddb-4ee5-883d-45049831782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hbdbscan_best_features_df = pd.DataFrame()\n",
    "hbdbscan_best_features_df['Features'] = best_hdbscan_features\n",
    "hbdbscan_best_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603704b6-bded-4600-987d-549c94f9898a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
